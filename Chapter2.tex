
\begin{equation}
\begin{array} {lllrr} 
\text{min} & \displaystyle\sum_{n = 1}^N \sum_{k = 1}^K \sum_{x \in C} g(x)^{f_k^i} Y_x^{f_k^i} \\\\
& \displaystyle\sum_{n = 1}^N \sum_{k = 1}^K \sum_{x \in C} T(x)^{f_k^i} Y_x^{f_k^i} & \leq &  D_{\textbf{F}} \\
& \displaystyle\sum_{n = 1}^N \sum_{k = 1}^K \sum_{x \in C} Y_x^{f_k^i} & = & 1 \\\\
& Y_x^{f_k^i} \in \lbrace 0, 1 \rbrace &&
\end{array}
\end{equation}


\begin{equation}
\begin{array} {lllrr} 
\text{min} & \displaystyle\sum_{\pi \in \Pi} \sum_{n = 1}^N \sum_{k = 1}^K \sum_{x \in C} g(x)^{f_k^i} Y_{(\pi, x)}^{f_k^i} \\\\
& \displaystyle\sum_{n = 1}^N \sum_{k = 1}^K \sum_{x \in C} T(x)^{f_k^i} Y_x^{f_k^i} & \leq &  D_{\textbf{F}} \\
& \displaystyle\sum_{\pi \in \Pi} \sum_{n = 1}^N \sum_{k = 1}^K \sum_{x \in C} Y_{(\pi, x)}^{f_k^i} & = & 1 \\\\
& Y_{(\pi, x)}^{f_k^i} \in \lbrace 0, 1 \rbrace &&
\end{array}
\end{equation}


\newpage

\section{Serverless Computing Paradigm}

In serverless computing platforms, computation is done by so-called \textit{function instances} which are completely managed by the serverless computing platform provider and act as tiny servers that can be invoked based on events forwarded by end-users \cite{PMSCP}.

Serverless computing platforms handle almost every aspect of the system administration tasks needed to deploy a workload on the cloud, providing a new simplified programming model according to which developers can focus on the business aspects of their applications only \cite{COSE}.

Moreover, the paradigm lowers the cost of deploying applications too, adopting a so-called “\textit{pay as you go}” pricing model, by charging for execution time rather than for allocated resources \cite{COSE}.


\subsection{Serverless Function}

A \textit{serverless function} represents a stateless, event-driven, self-contained unit of computation implementing a business functionality.

Although a serverless function generally represents a unit of executable code, submitted to FaaS platforms by developers using one or a combination of the programming languages supported by FaaS providers, a serverless function can be any cloud services eventually necessary to business logic, like cloud storage, message queue services, pub/sub messaging service etc.

When it represents executable code, developers can specify several configuration parameters, like timeout, memory size or CPU power \cite{COSE}.

A serverless function can be triggered through events or HTTP requests following which the FaaS provider executes it in a containerized environment, like container, virtual machine or even processes, using the specified configuration.

\subsection{Serverless Application}

In serverless paradigm, the most basic scenario is to invoke a single function through a HTTP request; however, if you want to built more complex applications, constructing so-called \textit{serverless application}, serverless functions must be connected and appropriately coordinated.

Formally, we define a serverless application as a stateless and event-driven software system made up of a serverless functions set hosted on one or more FaaS platforms and combined together by a so-called \textit{coordinator} (or \textit{orchestrator}).

Generally, the coordinator is a broker needed to implement the business logic of any application: it chains together serverless function, handles events among functions and triggers them in the correct order according to the business logic defined by developers. 

Most public cloud providers for serverless computing have introduced platforms to define and coordinate serverless function in order to built serverless application, like AWS Step Functions which combines multiple Lambda functions and other serverless services offered by AWS into responsive serverless applications

Although FaaS platforms continuously advance the support for serverless applications, existing solutions are dominated by a few large-scale providers resulting in mostly non-portable FaaS solutions 

and provider lock-in

occurs when transitioning data, products, or services to another vendor's platform is difficult and costly, making customers more dependent (locked-in) on a single cloud storage solution.

FC languages and runtime systems are still in their
infancy with numerous drawbacks. Current commercial and open
source offerings of FC systems are dominated by a few large-
scale providers who offer their own platforms, 

As FaaS platforms take over operational responsibilities,
besides uploading the source code of functions, users have
limited control over resources on FaaS platforms. Taking
AWS Lambda as an example, the amount of allocated mem-
ory during execution and the concurrency level are only op-
tions for tuning the performance of functions. The amount
of allocated memory is between 128 MB and 3,008 MB
in 64MB increments [13]. Previous researches have proven
that computational power and network throughput are in
proportion to the amount of allocated memory, and disk
performance also increases with larger memory size due
to less contention [14], [15]. By reserving and provisioning
more instances to host functions, high concurrency level can
decrease ﬂuctuations in the function performance incurred
by cold starts (container initialization provisioning delay if
no warm instance is available) and reduce the number of
throttles under very heavy request loads



\subsection{fdsfsd}




\begin{itemize}
	\item 
\end{itemize}

\subsection{title}


Serverless computing has given a much-needed agility to
developers, abstracted away the management and maintenance
of physical resources, and provided them with a relatively
small set of conﬁguration parameters: memory and CPU.
While relatively simpler, conﬁguring the “best” values for
these parameters while minimizing cost and meeting perfor-
mance and delay constraints poses a new set of challenges.
This is due to several factors that can signiﬁcantly aﬀect the
running time of serverless functions.


the run-time of these serverless
functions decreases with the increase of memory size allocated
to the function. However, the marginal improvement in the
run-time decreases as the memory increases.

This behavior
is because the pricing model as exposed by the cloud providers
is tightly coupled with the amount of resources speciﬁed
to execute the serverless function (c.f. Figure 1a), and the
dependency between memory and CPU resource allocation –
AWS Lambda allocates CPU power


a model for each executable belonging to a given choreography.



This helps developers decide if a developed
workload would comply with their QoS agreements, and if
not, how much performance improvement they would need
to do so. The performance improvement decided could be
achieved either by improving the design, quality of code, or
by simply resizing the resource allocated to each instance,
which is usually set by changing the allocated memory.


A cold start 

Cold/Warm start: as deﬁned in previous work [3], [8],
[10], we refer to cold start request when the request goes
through the process of launching a new function instance.
For the platform, this could include launching a new virtual
machine, deploying a new function, or creating a new
instance on an existing virtual machine, which introduces an
overhead to the response time experienced by users. In case
the platform has an instance in the idle state when a new
request arrives, it will reuse the existing function instance
instead of spinning up a new one. This is commonly known
as a warm start request. Cold starts could be orders of
magnitude longer than warm starts for some applications.
Thus, too many cold starts could impact the application’s
responsiveness and user experience [3].



his equation gives the probability of a request being
rejected (blocked) by the warm pool, assuming there are m
warm servers. If m is less than the maximum concurrency
level, the request blocked by the warm pool causes a cold
start. If the warm pool has reached the maximum concur-
rency level, any request rejected by the warm pool will be
rejected by the platform.

\subsection{Concurrency level}

Any public serverless computing platform imposes a limitation on the number of serverless function instance runnable at the same time by the corresponding platform provider; this limit is known as \textit{concurrency level}.

For example, in $2021$, AWS Lambda doesn't allow more than 1000 serverless function instances in running state at the same time.

\subsection{Start/Cold start}

When a request arrives on a FaaS platform, 

\begin{description}
	\item[Cold start] If the warm pool is busy, that is there are no serverless function instances in idle state able to serve an newly incoming request, latter will trigger the launching of a new function instance, which will be added to the warm pool. This event is called \textit{cold start}.
		
	From the FaaS provider point of view, this operation requires to start either a new virtual	machine or a new container; in any case, regardless of the method adopted, a cold start introduces a very important overhead to the response time experienced by users. 
	
	\item[Warm start] When there is at least one serverless function instance in idle state, the FaaS platform reuses it to serve the incoming request without launching a new one. This is called \textit{warm start}.
\end{description}  

A very important aspect is that cold starts could be orders of magnitude longer than warm starts for some applications; therefore, too many cold starts could impact the application’s responsiveness and user experience.

To be more precise, let $k \in \N$ and $C_{\text{max},P}$ the maximum concurrency level imposed by FaaS provider $P$, when a new request arrives on the platform, one of the following events can occur:

\begin{itemize}
	\item If $k < C_{\text{max},P}$, that is the number of function instances into the warm pool is less than the maximum concurrency level, the request will be blocked by the system causing a cold start.
	
	\item If $k = C_{\text{max},P}$, that is the warm pool size reaches the maximum concurrency level, any further request will be rejected by the platform
\end{itemize}

\subsubsection{Cold start probability}

As previously said, the rejection of a request by the warm pool triggers a cold start, adding subsequently a new function instance to the warm pool in order to handle received requests. 

To build our framework, we must to know the probability of a request being rejected by the warm pool; in other words, we must to compute the \textit{cold start probability}. 

Formally, let $k \in N$, the cold start probability $\textbf{P}_{C}$ is the probability that an arrival finds all $k$ function instances of the warm pool busy. Using Erlang-B formula, aforementioned probability can be calculated as follows:

\begin{equation}
	\textbf{P}_{C} = \dfrac{\dfrac{\rho^k}{k!}}{\displaystyle\sum_{j=0}^k \dfrac{\rho^j}{j!}}
\end{equation}




\section{Orchestrator Model}

\subsection{Resource owner}

A \textit{resource owner}, henceforward denoted with $R$, represents an entity capable of \textit{creating}, \textit{modifying} and \textit{authorizing} access to several resources of our system.

\newpage
\section{Serverless Choreography}

In this section we will introduce the notion of \textit{serverless choreography}, a very important \textit{resource} adopted to model and implement both serverless functions and serverless applications.

Informally, the notion of serverless choreography has been derived from that of a control-flow graph; as known, the latter describes, using graph notation, all paths that might be traversed through a serverless application during its execution. Similarly, a serverless choreography describes calling relationships between serverless functions which can be combined using several types of control-flow connectors.

\subsection{Preliminary Definitions}

Before to define what we mean for serverless choreography, is necessary to introduce some very useful notations.

Let $n \in \N$ and $G = (\Phi,E)$ a directed graph, where:

\begin{itemize}
	\item $\Phi$ is a finite set of vertices, such that $|\Phi| = n$;
	\item  $E \subseteq \Phi \times \Phi $ is a finite set of ordered pairs of vertices $e_{ij} = \left( \phi_i, \phi_j \right)$, where $\phi_i \in \Phi$ to $\phi_j \in \Phi$ for any $i,j \in \N \cap \left[ 1, n \right]$. Any ordered pair of vertices is also called directed edge;
\end{itemize}

Then, we will adopt following notations:

\begin{itemize}
	\item A \textit{path} of $G$ is defined as a finite sequence of distinct vertices and edges. We will denote a path by $\pi$ which formally can be represented as follows:
	
	\begin{equation}
		\pi = \phi_1 e_1 \phi_2 e_2 \ldots e_{n-2}\phi_{n-1} e_{n-1} \phi_n
	\end{equation}
	
	where:
	
	\begin{itemize}
		\item $\phi_i \in \Phi$, for all $i \in \N \cap \left[ 1, n \right]$
		\item $e_i = \left( \phi_i, \phi_{i+1} \right) \in E$, for all $i \in \N \cap \left[ 1, n-1 \right]$
	\end{itemize}

	\item Let $\phi_i,\phi_j \in \Phi$ for any $i,j \in \N \cap \left[ 1, n \right]$, the set denoted by $\Pi(\phi_i, \phi_j)$ identifies all possible paths starting from vertex $\phi_i$ and ending at vertex $\phi_j$.
		
	\item For any $u \in \N \cap \left[ 1, n \right]$, the set $out(\phi_u)$ ($in(\phi_u)$) denotes all edges starting (ending) from (to) vertex $\phi_u$, while the set $succ(\phi_u)$ ($pred(\phi_u)$) includes all direct successor (predecessors) vertices of $\phi_u$. Formally:
	
	\begin{eqnarray}\label{outDef}
		out(\phi_u) & \mathDef & \left\lbrace (\phi_u, \phi) \in E, \quad \forall \phi \in \Phi  \right\rbrace \\
		in(\phi_u) & \mathDef & \left\lbrace (\phi, \phi_u) \in E, \quad \forall \phi \in \Phi  \right\rbrace \\
		succ(\phi_u) & \mathDef & \left\lbrace \phi \in \Phi \mid (\phi_u, \phi) \in out(\phi_u)  \right\rbrace \\
		pred(\phi_u) & \mathDef & \left\lbrace \phi \in \Phi \mid (\phi, \phi_u) \in in(\phi_u)  \right\rbrace 
	\end{eqnarray}


\end{itemize}

\subsection{Definition}

According to the serverless paradigm, the execution of a serverless application starts with a particular function, which we will call \textit{entry point}, while any other serverless functions, belonging to the application, will be invoked subsequently according to specified business logic defined by developers; as we will see shortly, the latter can be naturally modeled by a weighted directed graph.

Clearly, the execution of a serverless application ends when the execution of the last function of the application ends; that ending function will be call as \textit{end point}. We assume that the entry point is unique.

Let $n \in \N \setminus \left\lbrace 0 \right\rbrace $ and $R$ a resource owner.

A \textit{serverless choreography} owned by $R$, or simply \textit{choreography} of $R$, is a resource represented by a weighted directed graph denoted as follows:

\begin{equation}
	\mathcal{C}_R \mathDef (\Phi,E)
\end{equation}

where:

\begin{itemize}
	\item $|\Phi| = n$;
	
	\item Each vertex $\phi \in \Phi$ is called \textit{abstract serverless function} and represents a computational unit; we will describe more in detail what we mean by abstract serverless function shortly;
	
	\item Let $i,j \in \N \cap \left[ 1, n \right]$ and $\phi_i, \phi_j \in \Phi$, any directed edge $e_{ij} = \left( \phi_i, \phi_j \right) \in E$ represents the calling relationship between two abstract serverless function which depends on the business logic defined by $R$. 
	
	In our context, any directed edge $\left( \phi_i, \phi_j \right) \in E$ states that the abstract serverless function $\phi_j$ \textit{can} be called by $\phi_i$;

	\item Let $i,j \in \N \cap \left[ 1, n \right]$, the number $p_{ij} \in \R \cap \left[ 0, 1 \right]$ is the weight assigned to the edge $\left(\phi_i, \phi_j \right) \in E$, where: 
	
	\begin{itemize}
		
		\item The number $p_{ij}$ represents the so-called \textit{transition probability} from $\phi_i$ to $\phi_j$, that is the probability of invoking $\phi_j$ after finishing the execution of $\phi_i$;
		
		\item We will use a function $P : \Phi \times \Phi \to \left[ 0, 1 \right]$, called \textit{transition probability function}, such that $P\left(\phi_i, \phi_j \right) = p_{ij}$. When $P\left(\phi_i, \phi_j \right) = 0$, it implies that the directed edge $\left( \phi_i, \phi_j \right) \notin E$, therefore $\phi_i$ cannot invoke $\phi_j$;
		
		\item For any path $\pi = \phi_1 e_1 \ldots e_{n-1} \phi_n$ of $\mathcal{C}_R$, we define \textit{transition probability of the path $\pi$} the following quantity:
		
		\begin{equation}
			TPP(\pi) = \prod_{i = 1}^{n-1} P\left(\phi_i, \phi_{i+1} \right)
		\end{equation}
		
	\end{itemize}
	
	\item $\Phi$ contains only one serverless abstract function, denoted by $\phi_{entry}$, acting as the entry point and at least one acting as the end point, which is denoted, instead, by $\phi_{end}$. Formally, we define aforementioned vertices as follows: 
	
	\begin{equation}
		\begin{array}{lcll}
			\phi \in \Phi & \text{ is the entry point of } \mathcal{S} & \Leftrightarrow & in(\phi) = \emptyset \\
			\phi \in \Phi & \text{ is the end point of } \mathcal{S} & \Leftrightarrow & out(\phi) = \emptyset
		\end{array}
	\end{equation}
	
	Then, following conditions must be hold:
	
	\begin{eqnarray}
		\exists !  \phi \in \Phi &\mid & in(\phi) = \emptyset \label{cond1} \\
		\exists   \phi \in \Phi & \mid & out(\phi) = \emptyset \label{cond2}
	\end{eqnarray}
	
	Finally, we will also use the notation $\alpha(\mathcal{C}_R)$ to denote the entry point $\phi_{entry}$ of a choreography $\mathcal{C}_R$. Conversely, we will adopt the notation $\omega(\mathcal{C}_R)$ to denote the set of all end points of $\mathcal{C}_R$
		
	\item Must be hold the condition according to which the transition probabilities of all paths between the entry point and	the end point of $\mathcal{C}_R$ sum up to $1$. Formally:
	
	\begin{equation}\label{cond3}
		\sum_{\phi_{end} \in \omega(\mathcal{C}_R)} \Big( \sum_{\pi \in \Pi(\phi_{entry}, \phi_{end})} TPP(\pi) \Big) = 1
	\end{equation}
	
	In other words, above conditions guarantees that any execution starting from $\phi_{entry}$ will terminate.
	
	\item A function, denoted by $D : \Phi \times \Phi \to \left[ 0, \infty \right)$, represents a delay function according to which $D\left(\phi_i, \phi_j \right)$ identifies the delay from $\phi_i$ to $\phi_j$ due to network delay or orchestration task;  
	
\end{itemize} 

A choreography $\mathcal{C}_R$ can be uniquely identified by an ordered pair $(a, b)$, where $a$ is the name of the resource owner $R$, while $b$ is the function choreography name.

Clearly, we say that choreography models a serverless function when $|\Phi| = 1$ and $|E| = 0$; conversely, it models a serverless function application when $|\Phi| > 1$ and $|E| > 0$.

From now, a choreography $\mathcal{C}_R$ will be briefly denoted by $\mathcal{C}$ when no confusion can arise about the resource owner $R$.

\subsubsection{Abstract Serverless Function}

Supposing that a choreography $\mathcal{C} = (\Phi,E)$ is given.

As said previously, any $\phi \in \Phi$ is called \textit{abstract serverless function}, or simply \textit{abstract function}, that is a \textit{resource} representing a computational unit required by business logic provided by developers.

According to our model, there are two types of abstract functions implementations:

\begin{itemize}
	
	\item $\phi$ is called \textit{serverless executable functions}, or simply \textit{executable function}, when $\phi$ contains executable code; therefore, in that case, we said that an executable function models a serverless function.
	
	$\mathscr{F_E}$ is defined as the set containing all executable function of $\mathcal{C}$ and it is formally defined as follows:
	
	\begin{equation}
		\mathscr{F_E} \mathDef \left\lbrace \phi \in \Phi \mid \phi \text{ is a serverless executable function }\right\rbrace 
	\end{equation}
	
	However, multiple different implementations of a given executable function can be provided by developers which, although they must be semantically and logically equivalent, may eventually expose different performance or cost behavior. 
	
	For any $\phi \in \mathscr{F_E}$, we will use $\textbf{F}_{\phi}$ notation to represent the so-called \textit{implementation-set} of $\phi$, that is the set containing any concrete implementation, denoted as $f_{\phi}$, of $\phi$.
	
	Later, we will explain how our framework is able to pick, for all $\phi \in \mathscr{F_E}$, exactly one $f_{\phi} \in \textbf{F}_{\phi}$ whose properties allow us to meet user-specified QoS objective.
	
	\item $\phi$ is called \textit{serverless orchestration functions}, or simply \textit{orchestration functions}, when $\phi$ contains the so-called \textit{orchestration code}. 
	
	According to our model, orchestration code represents the logic required to chain together any components of an application, handling events and triggering executable functions in the correct order according to the business logic. In other words, orchestration code is used to manage the control-flow of any application; later we will describe how it is possible.
	
	$\mathscr{F_O}$ is defined as the set containing all orchestration functions of $\mathcal{C}$ and it is formally defined as follows:
	
	\begin{equation}
		\mathscr{F_O} \mathDef \left\lbrace \phi \in \Phi \mid \phi \text{ is a serverless orchestration function }\right\rbrace 
	\end{equation}

\end{itemize}

Clearly, based on above definitions, we can say: 

\begin{eqnarray}
\mathscr{F_E} \cap \mathscr{F_O} & = & \emptyset \\
\mathscr{F_E} \cup \mathscr{F_O} & = & \Phi \\
|\mathscr{F_E}| + |\mathscr{F_O}| &=& |\Phi| 
\end{eqnarray}

Any abstract function $\phi$ is uniquely identified by an ordered pair $(a, b)$, where:
\begin{itemize}
	\item $a$ represents the identifier of the choreography $\mathcal{C}$;
	\item $b$ is the name of the abstract serverless function $\phi$;
\end{itemize}

\subsubsection{Executability condition}

Let $\mathcal{C} = (\Phi,E)$ a choreography.

Obviously, in order to effectively start the execution of a choreography, is required that, for each executable function $\phi \in \mathscr{F_E}$, \textit{at least one} concrete implementation $f_{\phi}$ exists.

Formally, we said that a choreography is \textit{executable} when: 

\begin{eqnarray}
	\label{eqn:SchedulabilityConditionOne}
	\mathcal{C} \text{ is executable } & \Leftrightarrow & |\textbf{F}_{\phi}| \geq 1 \qquad \forall \phi \in \mathscr{F_E}
\end{eqnarray}

We will only deal with executable choreographies. Moreover, for all $\phi \in \mathscr{F_E}$, we always assume that all $f_{\phi} \in \textbf{F}_{\phi}$ are already deployed on one or more FaaS platform by developers.

\subsubsection{Serverless Sub-choreography}

Let $\mathcal{C} = (\Phi,E)$ a choreography. 

The weighted directed sub-graph of $\mathcal{C}$ defined as follows:

\begin{equation}
	C^* \mathDef (\Phi^*,E^*) \qquad \text{ where } \Phi^* \subseteq \Phi \wedge E^* \subseteq E
\end{equation}

is called \textit{serverless sub-choreography} of $\mathcal{C}$, or simply \textit{sub-choreography} of $\mathcal{C}$, when the conditions \ref{cond1}, \ref{cond2} and \ref{cond3} are hold.

\subsubsection{Basic Serverless Choreography}\label{BasicDefinition}

Suppose to have a choreography $\mathcal{C} = (\Phi,E)$ satisfying following conditions:

\begin{eqnarray}
	|\mathscr{F_O}| = 0 \label{cond21} \\
	|\omega(\mathcal{C})| = 1 \label{cond22}
\end{eqnarray}

We said that any choreography $\mathcal{C}$, satisfying the conditions \ref{cond21} and \ref{cond22}, represents a \textit{basic serverless choreography}, or simply a \textit{basic choreography}.

\newpage
\section{Serverless Choreography Configuration}

Given a choreography $\mathcal{C} = (\Phi,E)$, the basic goal of our framework is to determine the so-called \textit{serverless choreography configuration}, called also \textit{choreography configuration} or, simply, \textit{configuration}, which allows us to meet user-specified QoS objectives.

Informally, a configuration specifies which concrete implementation $f_{\phi} \in \textbf{F}_{\phi}$, for all $\phi \in \mathscr{F_E}$, will be effectively executed, including several parameters, like memory size or CPU power.

\subsection{Executable Function Configuration}

Before explaining formally what we meant by choreography configuration, we must firstly to explain the concept of executable function configuration.

Let $m \in \N$, $\phi \in \mathscr{F_E}$ an executable function and $\textbf{F}_{\phi}$ the corresponding implementation-set.

Formally, an \textit{executable function configuration} for the executable function $\phi$ is a two-dimensional vector defined as follows:

\begin{equation}
x_{\phi} = (f_{\phi},m) \in \textbf{F}_{\phi} \times \N
\end{equation}

where:

\begin{itemize}
	\item $f_{\phi}$ represents a particular serverless function implementing the executable function $\phi$.
	\item $m$ represents the allocated memory size of $f_{\phi}$.
\end{itemize}

At this point, we can define some useful functions:

\begin{itemize}	
	\item $C_{\textbf{F}_{\phi}} : \textbf{F}_{\phi} \times \N \to \left[ 0, \infty \right)$ is the \textit{cost function} for any serverless functions belonging to the implementation-set $\textbf{F}_{\phi}$.
	
	Precisely, for all $f_{\phi} \in \textbf{F}_{\phi}$ and $m \in \N$, $C_{\textbf{F}_{\phi}}(f_{\phi}, m)$ returns the \textit{average cost} paid by developers to execute $f_{\phi}$ using an allocated memory size equal to $m$.
	
	
	\item $RT_{\textbf{F}_{\phi}} : \textbf{F}_{\phi} \times \N \to \left[ 0, \infty \right)$ is a \textit{delay function} for any serverless functions belonging to the implementation-set $\textbf{F}_{\phi}$.
	
	For all $f_{\phi} \in \textbf{F}_{\phi}$ and $m \in \N$, $RT_{\textbf{F}_{\phi}}(f_{\phi}, m)$ returns the \textit{average response time} when $f_{\phi}$ is invoked with memory size equal to $m$;
\end{itemize}




\subsection{Serverless Choreography Configuration}

Let $n,k \in \N \setminus \left\lbrace 0 \right\rbrace$ and a choreography $\mathcal{C} = (\Phi,E)$ such that $|\Phi| = n$ and $|\mathscr{F_E}| = k$ where $k \leq n$.

Formally, a serverless choreography configuration for $\mathcal{C}$ is a matrix $\textbf{X}$ such that:

\begin{equation}
\textbf{X} \mathDef \left\lbrace x_{\phi_{1}}, \ldots, x_{\phi_{k}} \right\rbrace \in \lbrace \textbf{F}_{\phi_{1}} \times \mathbb{N} \rbrace \times \ldots \times \lbrace \textbf{F}_{\phi_{k}} \times \mathbb{N} \rbrace = \Cross_{i = 1}^k \lbrace \textbf{F}_{\phi_{i}} \times \mathbb{N} \rbrace
\end{equation}

where, for all $i \in \left[ 1, k \right]$, $x_{\phi_{i}}$ is an executable function configuration for $\phi_{i}$.

%We will use $X(i)$ notation as a reference to the configuration $x_{\phi_{i}}$ while we will use $X(i,1)$ and $X(i,2)$ to denote respectively the implementation $f \in \textbf{F}_{\phi_i}$ and the allocated memory size $m \in N$ for the function $f$.

\section{The problem of cold starts}

In order to find a suitable serverless choreography configuration capable to satisfy QoS imposed by end users, we must find a way to decide  





\subsection{Metrics}

Let $\mathcal{C} = (\Phi,E)$ a basic choreography as defined in section \ref{BasicDefinition}.



Suppose to have a path $\pi = \phi_1 e_1 \phi_2 e_2 \ldots e_{n-2}\phi_{n-1} e_{n-1} \phi_n$ and a choreography configuration $\textbf{X}$ for $\mathcal{S}$. 

We can define the \textit{response time} of $\pi$ given $\textbf{X}$ as follows:

\begin{equation}
	RT_P(\pi, \textbf{X}) = \sum_{i = 1}^n \left( RT_{\phi_i} (X(i) \right)  + \sum_{i = 1}^{n-1} D(\phi_i,\phi_{i+1})
\end{equation}

Similarly, we define the cost of $\pi$ given $\textbf{X}$ as follows:

\begin{equation}
	C_P(\pi, \textbf{X}) = \sum_{\substack{1\le i\le n\\ \phi_i \in \mathscr{F_E}}} N(\phi_i) \cdot C_{\textbf{F}_{\phi_i}} (X(i))
\end{equation}


Then the response time and the cost of the choreography $\mathcal{S}$ a follows:

\begin{eqnarray}
	RT_C(\mathcal{S}, \textbf{X}) & = & \sum_{\pi \in \Pi(\phi_{entry}, \phi_{end})} TPP(\pi) \cdot RT_{P}(\pi, \textbf{X}) \\
	C_C(\mathcal{S}, \textbf{X}) & = & \sum_{\pi \in \Pi(\phi_{entry}, \phi_{end})} TPP(\pi) \cdot C_{P}(\pi, \textbf{X}) 
\end{eqnarray}



\section{Serverless Choreography Structures}

Supposing to have a serverless choreography $\mathcal{S} = (\Phi,E)$, we call \textit{structure} any sub-choreography $\mathcal{S}^*= (\Phi^*,E^*)$ whose entry point and the end point are, respectively, an opening and a closing orchestration functions of the same type. Formally:

\begin{equation}
	S^* \text{ is a structure } \Leftrightarrow \left\{ \begin{array}{rlll}
		\phi_{entry}^* & \in \mathscr{F_O^*} & \wedge & \mathscr{T}(\phi_{entry}^*) = \tau_{\alpha} \\ 
		\phi_{end}^* & \in \mathscr{F_O^*} & \wedge & \mathscr{T}(\phi_{end}^*) = \tau_{\omega}
	\end{array} \right.
\end{equation}

The most important aspect is that every structure can be viewed as a set of sub-choreographies of $\mathcal{S}^*$. Formally, suppose that $\Lambda = (A,B)$ is a graph such that:

\begin{equation}
\begin{array}{lll}
	A & \mathDef & \Phi^* \setminus \left\lbrace \phi_{entry}^*,\phi_{end}^* \right\rbrace   \\
	B & \mathDef & E^* \setminus \left( out\left(\phi_{entry}^*\right) \cup in\left(\phi_{end}^*\right) \right)
\end{array}
\end{equation}

Let $c \in \N$ and suppose that $c$ is the number of connected components of the graph $\Lambda$.

We denote by $\Theta_{\mathcal{S}^*}$ as the set containing all connected components of $\Lambda$ every of which is considered as a sub-choreography of $\mathcal{S}^*$. Formally:

\begin{equation}
	\begin{array}{c}
		\Theta_{\mathcal{S}^*} \mathDef \left\lbrace \theta_i, \ldots ,\theta_c \right\rbrace \\\\
		\text{ where } \\\\
		
		\theta_i \mathDef (\Phi^{**}_i, E^{**}_i) \text{ is a sub-choreography of } \mathcal{S}^* \\
		 \Phi^{**}_i \subset \Phi^{*} \wedge E^{**}_i \subset E^{*} \qquad \forall i \in \left[ 1, c \right] \\
	\end{array}
\end{equation} 

\subsubsection{Parallel}

A structure $\mathcal{P} = (\Phi,E,\Theta)$ is called \textit{parallel structure} when:

\begin{equation}
	TPP(\pi) = 1 \qquad \forall \pi \in \Pi(\phi_{start}, \phi_{end})
\end{equation}

Let $n \in \N$, supposing that $|\Theta| = n$, the response time of a parallel structure is the longest response time of all sub-choreographies $\theta_i \in \Theta$ for all $i \in \left[ 1, n \right]$, while its cost is equal to the sum their costs of execution. Formally, being $\textbf{X}$ a configuration of  $\mathcal{P}$:

\begin{equation}
	RT_{parallel}(\mathcal{P}, \textbf{X}) = max \left\lbrace RT_C(\mathcal{\theta}, \textbf{X}) \mid \theta \in \Theta \right\rbrace 
\end{equation}

\begin{equation}
	C_{parallel}(\mathcal{P}, \textbf{X}) = \sum_{\theta \in \Theta} C_C(\theta, \textbf{X})
\end{equation}



\subsubsection{Branch/Switch}

Let $\mathcal{B} = (\Phi,E,\Theta)$ a structure and suppose that $\alpha(\mathcal{B}) = \phi_{entry}$ and $\omega(\mathcal{B}) = \phi_{end}$. 

$\mathcal{B}$ is called \textit{branch structure} when following conditions are hold:

\begin{eqnarray}
	TPP(\pi) \neq 1  & \quad \forall \pi \in \Pi(\phi_{entry}, \phi_{end}) \\
    |\Theta| = n  & n \in \left\lbrace 1,2 \right\rbrace 
\end{eqnarray}

Let all $\theta_i \in \Theta$ sub-choreographies of $\mathcal{B}$, for all $i \in \left\lbrace 1,2 \right\rbrace $. Then the response time and the cost of a branch structure can be defined as follows:

\begin{equation}
	RT_{branch}(\mathcal{B}, \textbf{X}) = \sum_{i = 1}^n P(\phi_{entry}, \alpha(\theta_i)) RT_C(\theta_i, \textbf{X})
\end{equation}

\begin{equation}
	C_{branch}(\mathcal{B}, \textbf{X}) = \sum_{i = 1}^n C_C(\phi_{entry}, \alpha(\theta_i)) C_C(\theta_i, \textbf{X})
\end{equation}

Finally, according to our model, $\mathcal{B}$ is called \textit{switch structure} when $|\Theta| > 2$.

\subsubsection{Loop}

Let $\mathcal{L} = (\Phi,E,\Theta)$ a structure and suppose that $\alpha(\mathcal{L}) = \phi_{entry}$ and $\omega(\mathcal{L}) = \phi_{end}$. 

If $|\Theta| = 1$ and $(\phi_{end}, \phi_{entry}) \in E$, denoting by $\theta$ the unique sub-choreography of $\mathcal{L}$, $\mathcal{L}$ will be called \textit{loop structure} when following conditions are hold:

\begin{eqnarray}
	P(\phi_{start}, \alpha(\theta)) = 1 \\
	P(\phi_{end}, \phi_{entry}) \neq 0 \\
	P(\phi_{end}, \phi_{entry}) + \sum_{\phi \in succ(\phi_{end})} P(\phi_{end}, \phi) = 1
\end{eqnarray}

In order to compute the mean response time and the mean cost of a loop structure, we need to know $EI(\mathcal{L})$, that is the expected value of the number of iterations of $\mathcal{L}$.

We know that geometric distribution gives the probability that the first occurrence of success requires $k \in \N$ independent trials, each with success probability $p$ and failure probability $q = 1 - p$. In our case, if the our success corresponds to event "we will not execute the loop body", we know that success probability $p$ is given by:

\begin{equation}
	p = 1 - P(\phi_{end}, \phi_{entry})
\end{equation}

Then:

\begin{eqnarray}
	P(EI(\mathcal{L}) = k) & = & \Big[  1 - P(\phi_{end}, \phi_{entry}) \Big] \cdot \bigg[  P(\phi_{end}, \phi_{entry}) \bigg] ^{k-1} \\
	& = & pq^{k-1}
\end{eqnarray}

\begin{eqnarray}
	E[X] & = & \sum_{k = 1}^\infty (k-1) pq^{k-1} \nonumber \\
	& = & p \sum_{k = 0}^\infty kq^{k} \nonumber \\
	& = & p \cdot \frac{q}{(1-q)^2} \nonumber \\
	& = & \dfrac{q}{p} \nonumber \\
	& = & \dfrac{P(\phi_{end}, \phi_{entry})}{1 - P(\phi_{end}, \phi_{entry})} 
\end{eqnarray}

Then the response time and the cost of a loop structure can be defined as follows:

\begin{equation}
	RT_{loop}(\mathcal{L}, \textbf{X}) = E[I_{\mathcal{L}}] \cdot RT_C(\theta, \textbf{X})
\end{equation}

\begin{equation}
	C_{loop}(\mathcal{L}, \textbf{X}) = E[I_{\mathcal{L}}] \cdot C_C(\theta, \textbf{X})
\end{equation}


\section{Optimization}


\subsection{Performance Modeling}






We define the utility function as follows:

\begin{equation}
	F(\textbf{x}) \mathDef w_{RT} \cdot \dfrac{RT_{max} - RT(\textbf{x})}{RT_{max} - RT_{min}} + w_{C} \cdot \dfrac{C_{max} - C(\textbf{x})}{C_{max} - C_{min}}
\end{equation}

where $w_{RT}, w_{C} \geq 0$, $w_{RT} + w_{C} = 1$, are weights for the different QoS attributes, while $RT_{max}$ ($RT_{min}$) and $C_{max}$ ($C_{min}$) denote, respectively, the maximum (minimum) value for the overall expected response time and cost.


\begin{equation}
\begin{array} {rll} 
\displaystyle \operatorname*{arg\,max}_\textbf{X} & F(\textbf{X}) \\\\
\text{subject to} & C(\textbf{X}) \leq C_{user} \\\\
	              & RT(\textbf{X}) \leq RT_{user} \\\\              
	              & \displaystyle \sum_{j = 1}^{|\textbf{F}_{\phi_{i}} \times \N|} x_{\phi_{i_{j}}} = 1 & \qquad \forall \phi_{i} \in \mathscr{F_E} \\\\
				  & x_{\phi_{i_{j}}} \in \lbrace 0, 1 \rbrace & \qquad \forall \phi_{i} \in \mathscr{F_E} \wedge j \in \N \cap [1,|\textbf{F}_{\phi_{i}} \times \N|]
\end{array}
\end{equation}

fdsfsdfsdfsd

\begin{equation}
\displaystyle 
\sum_{i = 0}^{|\mathscr{F_E}|} 
\sum_{j = 0}^{|\textbf{F}_{\phi_i}|} 
\sum_{s \in M \subset \N } 
F(x_{\phi_{0}}, \ldots, x_{\phi_{|\mathscr{F_E}|}}) 
\end{equation}


% $\phi \in \mathscr{F_E}$













