\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage[mathscr]{euscript}
\usepackage{tikz}

\newcommand{\Cross}{\mathbin{\tikz [x=1.8ex,y=1.8ex,line width=.15ex] \draw (0,0) -- (1,1) (0,1) -- (1,0);}}

\newcommand*{\N}{\mathbb{N}}
\DeclareMathOperator*{\argminA}{arg\,min}

\newcommand{\mathDef}{\overset{\textit{def}}{=}}

\usepackage{hyperref}
\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}

\theoremstyle{definition}

\newtheorem{definition}{Definition}[section]
\newtheorem*{remark}{Remark}

\begin{document}
	
\bibliographystyle{plain}
\bibliography{Bibliography.bib}


\tableofcontents
\newpage

\input{Chapter1}
\input{Chapter2}

\section{Super-peer Node}


An \textit{end-user} represents, instead, a third-party application that wants execute  one or more function choreographies.

\section{Peer Node}

Proposed 

Hybrid structures are notably deployed in collaborative distributed systems.
The main issue in many of these systems is to first get started, for which often
a traditional client-server scheme is deployed. Once a node has joined the
system, it can use a fully decentralized scheme for collaboration.


Relating to a specified function choreography $X$ belonging to resource owner $R$, a peer $P$ of our system can be in one of the following states:

\begin{description}
\item[Active State] When $P$ has been marked as responsible for manage all invocation requests of $X$ forwarded by end users.
\item[Forwarder State] Otherwise
\end{description}


function choreographies (FCs) or workflows of
functions. 

As known, in server-less computing platforms, computation is done in \textbf{function instances}. These instances are completely managed by the server-less computing platform provider (SSP) and act as tiny servers where a function is been executed.


\section{Resources}





\section{System's resources and actors}



Given a resource owner $R$, there are two type of resources which he can manage:

\begin{enumerate}
\item Function choreographies.
\item Server-less function implementations (also called \textit{concrete server-less functions})
\end{enumerate}



\subsection{Server-less function swarms}

Informally, a so-called \textit{server-less function swarm} represent a set of concrete server-less function with very specific properties.

Precisely, let $l \in \mathbb{N}$ such that $l \neq 0$, $R$ a resource owner, $P$ a server-less computing platform provider and  $X_R$ a set of concrete server-less functions. Moreover, let $\textbf{X}_R$ the set containing all concrete function implementations defined and deployed by $R$ on any provider. 

A set $X_R \subseteq \textbf{X}_R$ is called a \textit{server-less function swarm}, or simply \textit{swarm}, if:

\begin{enumerate}
\item $|X_{R}| = n \geq 1$, that is $X_{R}$ must contain at least one concrete function.
\item $X_{R}$ contain concrete functions that share the same platform provider $P$ where they will be executed.
\item $X_{R}$ contain concrete functions that share the same limit $l$ in term of max number of server-less function instance runnable at the same time by the corresponding platform provider $P$. That limit is also called \textit{server-less function swarm's concurrency limit}, or simply, \textit{concurrency limit}. 
\end{enumerate}

Is very important to make clear that only at most $l$ concrete functions belonging to $X_R$ can be executed simultaneously by $P$. The value of $l$ depends by specific policies adopted by $P$; some of them imposed that limit \textit{per-account}, others \textit{per-functions}. Our model supports both approaches because:

\begin{itemize}

\item If $P$ imposed limits \textit{per-function}, then $|X_{R}| = 1$, that is, $X_{R}$ will contain only one function defined and deployed by $R$ in $P$, where $l$ will be represent the provider's per-function limit.

\item If $P$ imposed a limit \textit{per-account}, then generally $|X_{R}| \geq 1$ and it include all concrete server-less function deployed on $P$ while $l$ will be represent the provider's global limit. 
\end{itemize}

\subsubsection{Server-less function sub-swarms}

A sub-swarm of ${X_{R}}$, which we will denote with $\Delta_{X_{R}}$, is the term with which we denote any element belong to the power set\footnote{The \textit{power set} $\mathcal{P}(S)$ of a set $S$ is the set of all subsets of $S$, including the empty set and $S$ itself.} of $X_{R}$, excluding the empty set. Formally, any $\Delta_{X_{R}} \in \mathcal{P}(X_{R}) \setminus \oslash$ is a sub-swarm.

Is very important to remember that in our model any sub-swarm $\Delta_{X_{R}}$ of ${X_{R}}$ has the \textit{same} concurrency limit of ${X_{R}}$.

\section{Function choreography scheduler}

\subsection{Schedulability condition}

Let $FC_R$ a function choreography belonging to a resource owner $R$ and $F_{abstract}$ its server-less abstract functions set. Moreover, be $\textbf{X}_R$ the set containing all functions deployed by $R$ in any provider.

In order to effectively start the execution of a function choreography, is required that for each abstract function $f_{abstract} \in F$ \textit{at least one} concrete function $f$, which implements it, exists.

Formally, a function choreography is said \textit{schedulable} when: 
\begin{equation}
\label{eqn:SchedulabilityConditionOne}
\begin{array}{lc}

& \forall f_{abstract} \in F_{abstract} \\

FC_R \text{ is schedulable } \Leftrightarrow & \\
 & \exists f \in \textbf{X}_R \mid  f \text{ implements } f_{abstract} \\
\end{array}
\end{equation}

Although it is correct, the condition expressed by equation \ref{eqn:SchedulabilityConditionOne} is not very precise, because $\textbf{X}_R$ can contain some functions that doesn't implement any $f_{abstract} \in F_{abstract}$.

Therefore, we define $\Omega_{FC_R}$ as the set containing only concrete functions that are needed to execute $FC_R$, which can both to belong to any provider and to have different concurrency limits.

Since multiple implementations of a same abstract function can exist at the same time, we can exploit the notion of swarm and sub-swarm to formally define the set $\Omega_{FC_R}$.

Let $n \in \mathbb{N}$, such that $n \geq 1$, and $X_{R_i}$ the $i$-th swarm and $\Delta_{X_{R_i}}$ its sub-swarm which contains only concrete functions implementing one or more $f_{abstract} \in F_{abstract}$, where $1 \leq i \leq n$.

We define $\Omega_{FC_R}$ as follows: 

\begin{equation}
\begin{array}{c}
\Omega_{FC_R} \mathDef \Delta_{X_{R_1}} \cup  \ldots \cup \Delta_{X_{R_n}} = \bigcup_{i = 1}^n \Delta_{X_{R_i}} \\

\text{ where } \\	

\Delta_{X_{R_i}} \cap \Delta_{X_{R_j}} = \oslash \quad \text{ for } i,j \in \left[ 0, n \right] \mid i \neq j \\

\forall f \in \Delta_{X_{R_s}} \quad f \text{ implements } f_{abstract} \text{ for } s \in \left[ 0, n \right] \\

\end{array}
\end{equation}

Since belong to different swarms, please note that any $\Delta_{X_{R_i}}$ and $\Delta_{X_{R_j}}$, for any $i \neq j$, can belong to the same provider but they cannot share the same concurrency limit.

Generally the schedulability condition for $FC_R$ can be written as follows:

\begin{equation}
FC_R \text{ is schedulable } \Leftrightarrow \exists \Omega_{FC_R}
\end{equation}


\subsection{The $\Delta_{X_{R}}$-Scheduler}

Let $R$ a resource owner, $P$ the server-less provider and $\Delta_{X_{R}}$ a sub-swarm of a $X_{R}$, where $k$ its concurrency limit.

Let $m \in \mathbb{N}$, a \textit{$\Delta_{X_{R}}$-Scheduler}, denoted as $S_{(\Delta_{X_{R}},m)}$ represents a queuing system, implementing any scheduling discipline, equipped with $m$ so-called \textit{virtual function instance}, where $m \leq k$.

Its aim is to decide when and which function, belonging to $\Delta_{X_{R}}$, must be performed on $P$. 

The parameter $m$ is also called \textit{scheduler capacity}.

\subsubsection{Virtual function instance}

A \textit{virtual function instance} represents a real function instances, clearly belonging to the server-less computing platform provider, which is \textit{virtually} owned by $S_{(\Delta_{X_{R}},m)}$.

Therefore, $m$ represents the max number of server-less function instances usable simultaneously by $S_{(\Delta_{X_{R}},m)}$.

\subsubsection{Proprieties and constrains}

According to our model, a $\Delta_{X_{R}}$-scheduler capable to manage any function belonging to $\Delta_{X_{R}}$, if exist, is \textit{not} unique, although it is unique inside a peer node. 

In order to achieve better performance in terms of network delay experienced by end users, fault tolerance and load balance, any peer nodes can hold a $\Delta_{X_{R}}$-scheduler in order to manage incoming request sent by several users spread in different geographic regions. 

However, despite there is no upper bound to the number of $\Delta_{X_{R}}$-schedulers existing at the same time in our system, there is a limitation regarding the scheduler capacity of each existing scheduler. 

Let's start summarizing all rules regarding $\Delta_{X_{R}}$-schedulers:

\begin{enumerate}

\item All peer node of our system can hold a $\Delta_{X_{R}}$-scheduler object.

\item Each node can hold only one instance of type $\Delta_{X_{R}}$-scheduler.

\item Let $n \in \mathbb{N}$ such that $n \geq 1$, suppose that our system contains $n$ peer nodes holding a $\Delta_{X_{R}}$-scheduler.

To be more precise, let's say that a sequence $S_{(1,(\Delta_{X_{R}},m_1))}, \ldots , S_{(i,(\Delta_{X_{R}},m_n))}$ exist at the same time in our system, where $S_{(i,(\Delta_{X_{R}},m_i))}$ represent the $\Delta_{X_{R}}$-scheduler owned by $i$-th node having scheduler capacity equal to $m_i$.

Following constraint must be hold:

\begin{equation}
\label{eqn:SchedulerConstrains}
\sum_{i=1}^{n} m_i \leq k
\end{equation}

where $k \in \mathbb{N}$, with $k > 0$, is the concurrency limit of the swarm $X_R$.

Remember that any sub-swarm $\Delta_{X_{R}}$ share the same concurrency limit of $X_R$. Therefore, equation \ref{eqn:SchedulerConstrains} states that, the sum of all scheduler capacities which manage the functions belonging to $\Delta_{X_{R}}$, must be less or equal to the max number of function instances executable at the same time on the server-less computing platform provider.

\end{enumerate}

\subsection{The $FC_R$-Scheduler}

To support hybrid-scheduling, that is the ability to execute multiple concrete function implementations belonging to different providers or subjected to different concurrency limit, in order to select the most suitable concrete function implementation according to a given QoS, unfortunately only one ``\textit{scheduler}" is not enough.

We call \textit{$FC_R$-Scheduler} a set of \textit{$\Delta_{X_{R}}$-Schedulers} where $\Delta_{X_{R}} \in \Omega_{FC_R}$. Is always required that $|FC_R| \geq 1$, that is, at least one scheduler must exist.

\section{$FC_R$-Active Peer Node}

According to our model, in order to effectively invoke all server-less concrete function belonging to a function choreography $FC_R$, is required that a peer node is ``active".

We said that a peer node $A$ is $FC_R$-\textit{active peer node}, or, simply, \textit{active}, when it holds a $\Delta_{X_{R}}$-scheduler for any sub-swarm in $\Omega_{FC_R}$. Formally:

\begin{equation}
A \text{ is } FC_R\text{-active peer node } \Leftrightarrow \forall \Delta_{X_{R}} \in \Omega_{FC_R} \quad \exists S_{(\Delta_{X_{R}},m)} \text{ hold by } A
\end{equation}

Multiple nodes can be active at the same time. Any node perform its scheduling decision independently.


\section{Architecture overview}

Our system design is based on a network of nodes, or \textit{peers}, every of which has the \textit{same functionality}; in fact, any of them is able to handle request submission, request scheduling and, potentially, request execution. 

This is the reason according to which we can mark our proposal as a \textit{P2P system}.

\subsection{Overlay network}

Our system's nodes are connected by an \textit{overlay network}.

\begin{definition}[Overlay network]
An overlay network is a virtual network built on top of a physical network according to which each nodes can communicate with other if and only if they are connected by virtual links belonging to the virtual network. 
\end{definition}

\begin{remark}
A node may not be able to communicate directly with an arbitrary other node although they can communicate through physical network.
\end{remark}

To be more precise, we have adopt a fully \textit{centralized unstructured overlay network} because the \textit{peer-resource index}, sometimes called \textit{directory}, is centralized. 

Please note that hybrid unstructured or fully decentralized solutions are technically possible, but guarantees about quality of service are very difficult.

\subsubsection{Locality-awareness property}

\textit{Locality-awareness} is one of the essential characteristics of our system. In fact, if each peer is able to select his neighbours exploiting a suitable locality aware algorithm, is possible to decrease user experienced delays.

We have decided to adopt a multi-level based locality-aware neighbour selection called \textit{intra-AS lowest delay clustering algorithm} (ASLDC). 

When ASLDC algorithm is used, each peer chooses nearby peers only from those within the same AS; then it ranks its neighbours in terms of transmission latency, preferring to o establish the connection with the node with the shortest latency to itself. 

TODO \cite{UnderstandingLocalityAwareness}

\subsection{Fault Tolerance}

Like in many other P2P implementation, there are two ways of detecting failures in our proposed solution:

\begin{enumerate}
\item If a node tries to communicate with a neighbour and fails.
\item Since all nodes send to all his neighbour nodes so-called ``heartbeat" messages, that is messages sent at fixed time intervals to indicate that the sender is alive, is possible to detect a failure by not receiving aforementioned periodic update messages after a long time. 

\end{enumerate}

\subsubsection{Availability}

Let $n \in \mathbb{N}$, $R$ a resource owner and $FC_R$ his function choreography, since multiple $FC_R$-Scheduler object can exist in $n$ different nodes, our proposed solution can guarantee an high degree of availability. 

In fact, when a node node holding an $FC_R$-Scheduler fails, all end users requests related to $FC_R$ can be routed to any other node holding a $FC_R$-Scheduler. 

\subsubsection{Replication}

Although multiple $FC_R$-Scheduler object can exist, it's not mean that $FC_R$-Scheduler object are replicated, because every of them manage different virtual function instances. 

Except the peer-resource index, any form of replication is performed by our system.







\begin{algorithm}
\caption{An algorithm with caption}\label{alg:cap}
\begin{algorithmic}

\State $y \gets 1$


\end{algorithmic}
\end{algorithm}




 and $\Delta_{X_{R}}$-Scheduler








The locality awareness of the overlay network is used in scheduling jobs, described in Section 3.2







physical network but with added properties such as fault tolerance and flexibility. 











The first problem is scheduling. Since there
is no central scheduler it is difficult



Hybrid unstructured overlay


\subsection{$FC_R$-Request}







 object with only one limitation:

Suppose that globally there are a set of schedulers $S_{1,({R_{X}},m_1)}, \ldots , S_{p,({R_{X}},m_p)}$, where $p \in \mathbb{N}$ with $p \geq 1$



To be more precise, when a function $x_j$ must to be execute, let $s$ the current number of busy virtual instances, one of the following events may occur:
\begin{enumerate}
\item if $s < m$, the scheduler invoke directly the function $x_j$ on the provider.
\item if $s = m$, the scheduler delay the execution of the function $x_j$ on the provider according to implemented scheduling discipline.
\end{enumerate}








Let $R$ a resource owner and $R_x$ its function choreography made up of $R_{x_1}, R_{x_2}, \ldots, R_{x_n}$ unique server-less functions; it is said that a peer node $P$ is \textbf{responsible} for $R_x$ when it contains a sequence of schedulers $S_{R_1}, S_{R_2}, \ldots, S_{R_k}$ with $k \leq n$, belonging to $R$, capable to invoke all server-less function belonging to $R_x$. 

It is said that a 

Depending on the definition of the function choreography provided by $R$ and the unique characteristics of back-end server-less providers which execute all serverless functions $R_{x_n}$ of 




It is said that a scheduler $S$ is capable to invoke a server-less function when 
, a scheduler $S$ can invoke multiple




When a peer $A$, placed ``\textit{at the edge}" of the network, receives a new request of invocation for $X$ by an end user, it performs following task in that order:

\begin{enumerate}
\item If it responsible It check for it is an already an \textit{active peer} to manage 
\end{enumerate}




 has found the tracker for a file F, the tracker returns a subset
of all the nodes currently involved in downloading F.




Replication and Fault Tolerance. There are two ways of detecting failures in CAN,
the first if a node tries to communicate with a neighbor and fails, it takes over that neighborâ€™s
zone. The second way of detecting a failure is by not receiving the periodic update message
after a long time. In the second case, the failure would probably be detected by all the
neighbors, and all of them would try to take over the zone of the failed node, to resolve this,
all nodes send to all other neighbors the size of their zone, and the node with the smallest
zone takes over.

\newpage


\begin{equation}
E[T] = \sum_{i = 0}^n E[S_i] + E[T_{Q_i}]
\end{equation}

\section{Queuing system of a $\Delta_{X_{R}}$-scheduler}

Let $k \in \mathbb{N}$ such that $k \geq 0$. Moreover, suppose that $R$ represents a resource owner, $P$ a server-less provider and $\Delta_{X_{R}}$ a sub-swarm of the swarm $X_{R}$ where $k$ its concurrency limit. Finally, let $S_{(\Delta_{X_{R}},m)}$ a $\Delta_{X_{R}}$-scheduler. 

Obliviously, since there are only $m$ available virtual function instances, if there are more than $m$ server-less functions waiting to be execute, a choice has to be made about which server-less function has to run next to ensuring QoS guarantees for latency critical applications.

Because we expect to execute server-less functions with different response-time requirements, which may have different scheduling needs, i have designed the $\Delta_{X_{R}}$-scheduler as a queuing system implementing a \textit{multilevel queue scheduling algorithm} which partitions the ready queue into several separate queues. 

According to our solution, each queue has its own scheduling algorithm and any server-less function is permanently assigned to one queue according to his class.

In addition, we have adopt \textit{round-robin} (\textit{RR}) scheduling algorithm to perform scheduling activity among the queues.

\subsection{Server-less function preemption}

A very important consideration regards server-less function preemption. 

In our context, due to FaaS paradigm, which hides the complexity of servers where our functions will be executed, the ability to preempt functions is not naturally available.

For that reason, only non-preemptive scheduling algorithms, according to which once a server-less function starts running it cannot be preempted, even if a higher priority server-less function comes along, can be adopted.

Since many scheduling algorithms require job preemption to run optimally, this situation can lead to a suboptimal resource management. 

\subsection{Queuing system design}

A $\Delta_{X_{R}}$-scheduler is made up of three queues:

\begin{itemize}

\item One queue implements a \textit{Non-Preemptive Least-Slack-Time-First} (\textit{LST}) scheduling algorithm. We will refer to that queue using $Q_{LST}$ notation.

That algorithm implements a \textit{dynamic priority scheduling approach} where priorities are assigned to server-less functions based on their \textit{slacks}.

At any time $t$, the \textit{slack} of a job with deadline at $d$ is equal to $d - t - s$, where $s$ is the time required to complete the job. 

Any job having strict latency requirements must to be assign to this queue.

\item Another queue implements, instead, a \textit{Non-Preemptive Shortest-Job-First} (\textit{SJF}) scheduling algorithm. That queue is denoted with $Q_{SJF}$.

That policy assigns priorities to jobs based on their size: the smaller the size, the higher the priority.

This queue is reserved for any function choreography that tolerates high latency. However, according to queueing theory, since SJF performs very bad when service time distribution is heavy-tailed; in fact

\begin{equation}
E[T_Q(x)]^{SJF} = \dfrac{\rho E[S^2]}{2E [S]} \cdot \dfrac{1}{(1 - \rho x )^2}
\end{equation}

This queue is not suitable for very, very large server-less function. The variance in the job size distribution must be low.

\item Finally, the third queue exploits the the simplest scheduling algorithm, that is the \textit{First-Come-First-Served} (\textit{FCFS}) policy, and it is denoted using $Q_{FCFS}$ notation.

This queue is used for any heavy-tailed server-less function which can tolerate high latency.
\end{itemize}


\subsection{The virtual function instances allocation problem}

Let $n \in \mathbb{N}$ such that $n \geq 1$ and suppose that, at a certain time, a sequence including $n$ unique $\Delta_{X_{R}}$-schedulers exist in our system.

Formally, that sequence is denoted with $S_{(1,(\Delta_{X_{R}},m_1))}, \ldots , S_{(i,(\Delta_{X_{R}},m_n))}$, where $S_{(i,(\Delta_{X_{R}},m_i))}$ represents the $\Delta_{X_{R}}$-scheduler owned by $i$-th node having scheduler capacity equal to $m_i$.

We have already established that the following constraint must be hold:

\begin{equation}
\label{eqn:SchedulerConstrains}
\sum_{i=1}^{n} m_i \leq k
\end{equation}

where $k \in \mathbb{N}$, with $k > 0$, is the concurrency limit of the swarm $X_R$.


\subsubsection{Reactive scaling policy}

Every node of the system monitors all queues belong to its $\Delta_{X_{R}}$-Scheduler and adjusts the number of reserved virtual function instances to preserve quality-of-service guarantees. This activity is called \textit{reactive scaling policy}.

Although a $S_{(\Delta_{X_{R}},m)}$ owns $m$ virtual function instances, they are not equally shared between aforementioned run queue. 

First of all, let be $v_{LST}$, $v_{SJF}$ and $v_{FCFS}$ the numbers of the virtual function instances bounded to the $Q_{LST}$, $Q_{SJF}$ and $Q_{FCFS}$ run-queue. 

At any time, following constrains must be hold:

\begin{equation}
v_{LST} \mathDef \dfrac{N \cdot E[S]}{\sum slack} 
\end{equation}

\begin{equation}
v_{SJF} \mathDef \dfrac{len(Q_{SJF})}{\alpha} 
\end{equation}

\begin{equation}
v_{FCFS} \mathDef \dfrac{len(Q_{FCFS})}{\alpha} 
\end{equation}

\begin{equation}
m = v_{LST} + v_{SJF} + v_{FCFS}
\end{equation}

\begin{enumerate}
\item If the current number of virtual function instances is not enough to avoid deadline misses by latency constrained server-less function inside $Q_{LST}$, will be add to $VCPU_{LST}$ a number of virtual processor equal to 

\begin{equation}
v_{LST} < \dfrac{N \cdot E[S]}{\sum slack} 
\end{equation}


\end{enumerate}



This form of aging prevents starvation. 


Finally, suppose that a node peer $A$ holds a $S_{(\Delta_{X_{R}},m_A)}$, which represents the 
$\Delta_{X_{R}}$-Scheduler object.

\begin{enumerate}
\item Once a request is received over HTTP
\item Ask to the coordinator for 
\end{enumerate}


\begin{equation}
m = \dfrac{\sum slack}{E[S] \cdot m_{current}} + \dfrac{N}{\alpha}
\end{equation}

\begin{equation}
\lambda_{FC_R} \geq \lambda_{threshold} 
\end{equation}


\begin{equation}
RTT_{(A,B)} > RTT_{(A,C)} + 
\end{equation}

\begin{equation}
f_{abstract} = next 
\end{equation}

\begin{equation}
f_{abstract} = min(E[f_{\Delta_{X_{R_i}}}] + E[T_{Q_{LST}}] F
\end{equation}






\subsubsection{Server-less function scheduling for QoS ``Minimum Response Time"}

Let $n \in \mathbb{N}$, such that $n \geq 1$, supposing to have an $FC_R$-\textit{active peer node} and let $\Omega_{FC_R}$ the set of all sub-swarms containing at least one function implementing any $f_{abstract} \in \textbf{F}_{abstract}$, where $|\Omega_{FC_R}| = n$.

Finally, suppose that $f_{abstract}$ is the next abstract function that have to be executed according to the control-flow logic described by $FC_R$.

\begin{enumerate}
\item For $1 \leq i \leq n$, let $\Delta_{(X_{R_i}, f_{abstract})} \subseteq \Delta_{X_{R_i}}$ containing all concrete functions implementing $f_{abstract}$.

\item For $1 \leq i \leq n$, select $\textbf{f}_i \in \Delta_{(X_{R_i}, f_{abstract})}$ functions having the minimum response time $E[T_{\textbf{f}_i}]$.

\item Found $i \in [0,m]$ such that: 

\begin{equation}
E[T_{\textbf{f}_i}] + E[T_{Q_i}]^{\textbf{LST}} < E[T_{\textbf{f}_j}] +E[T_{Q_j}]^{\textbf{LST}} \quad \text{ for } j \in \mathbb{N}, 1 \leq j \leq n, i \neq j
\end{equation}

\item Select the i-th \textit{$\Delta_{X_{R}}$-Schedulers} belonging to \textit{$FC_R$-Scheduler} of the node and add  $\textbf{f}_i$ into the $Q_{LST}$ run-queue.

\end{enumerate}




















\end{document}