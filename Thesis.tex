\documentclass[12pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{natbib}
\usepackage{titlesec}
\usepackage{tikz}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{frontespizio} 
\usepackage[mathscr]{euscript}
\usepackage[a4paper,left=3.5cm,right=2.5cm]{geometry}
\usepackage[ruled,linesnumbered]{algorithm2e}
\SetKw{KwBy}{by}

\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\onehalfspacing

% Custom commands...
\newcommand{\Cross}{\mathbin{\tikz [x=2.5ex,y=2.5ex,line width=.10ex] \draw (0,0) -- (1,1) (0,1) -- (1,0);}}
\newcommand{\mathDef}{\overset{\textit{def}}{=}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Rplus}{\mathbb{R}^+}
\newcommand{\SetMinusZero}{\setminus \left\{0\right\}}
\newcommand{\SetFromOneTo}[1]{\N \cap \left[1,#1\right]}
\newcommand{\SetFromZeroToOne}{\left[0,1\right]}
\newcommand{\ItalicQuotMark}[1]{``\textit{#1}"}


\hypersetup{
    colorlinks=true, %set true if you want colored links
    linktoc=all,     %set to all if you want both sections and subsections linked
    linkcolor=black,  %choose some color if you want links to stand out
}


\begin{document}
	
\begin{frontespizio} 
	\Universita{Roma ``Tor Vergata'' } 
	\Logo[3cm]{./images/logo}
	\Facolta{Ingegneria} 
	\Corso[Laurea Magistrale]{Ingegneria Informatica} 
	\Annoaccademico{2020--2021} 
	\Titolo{A QoS-aware broker \\ for multi-provider serverless applications}
	\NCandidato{Studente} 
	\Candidato[0273395]{Andrea Graziani} 
	\NRelatore{Docente}{} 
	\Relatore{Prof.ssa Valeria Cardellini}
	\Correlatore{Dott. Gabriele Russo Russo}
\end{frontespizio} 
	
	
\tableofcontents

\chapter{Introduction}

\section{The Serverless Computing Paradigm}

In recent years, thanks to an increased popularity of several lightweight virtualization solutions, specifically containers and container-orchestration systems, many public cloud providers have launched a new set of cloud computing services belonging to new service's type category called \textit{Function-as-a-Service} (\textit{FaaS}), where almost every aspect of the system administration tasks needed to deploy a workload on the cloud, like provisioning, monitoring, resource management and scaling, are directly managed by the provider with a minimum involvement of the user.

With the development of FaaS platforms, the \textit{serverless computing paradigm}, a new application service model according to which cloud applications are abstracted as a group of so-called \textit{serverless functions}, hosted and orchestrated by FaaS platforms, has emerged. 

A serverless function represents a stateless, event-driven, self-contained unit of computation implementing a business functionality.

Although a serverless function generally corresponds to an unit of executable code submitted to FaaS platforms by developers using one or a combination of the programming languages supported by FaaS providers, it can represent any cloud services eventually necessary to business logic, like cloud storage, message queue services, pub/sub messaging service etc.

A serverless function can be triggered through events or HTTP requests following which the FaaS provider takes care of its execution inside a containerized environment called \textit{function instance}, like container, virtual machine or even processes, using a user specified \textit{serverless function configuration}, which include several parameters, like timeout, memory size or CPU power \cite{COSE}. Function instances acts as tiny servers completely managed by the serverless computing platform. 

Despite the most basic scenario is represented by the invocation of a single function, when a more complex application is needed, serverless functions can be connected and appropriately orchestrated in a \textit{workflow}, obtaining a so-called \textit{serverless application}.

Generally, we can define a serverless application as a stateless and event-driven software system made up of a serverless functions set hosted on one or more FaaS platforms and combined together by a so-called \textit{coordinator} (or \textit{orchestrator}). The latter is usually represented by is a broker, needed to implement the business logic of any application; it chains together serverless function, handles events among functions and triggers them in the correct order according to the business logic defined by developers. 

Nowadays, there are many public cloud platforms providing serverless computing services to deploy and execute serverless functions, including AWS Lambda \ref{}, IBM Cloud Function \ref{}, Microsoft Azure Functions \ref{} and Google Cloud Functions \ref{}. Moreover, some providers have even introduced FaaS platforms acting as coordinator to build serverless application; AWS Step Functions is an example which allows to combine multiple Lambda functions, including other serverless services offered by AWS, like the storage service Amazon S3, into an applications.  

Clearly, a very important advantage of serverless computing is a new simplified programming model, according to which developers can focus on logic and business aspects of their applications only, without worry about the server management, which can lead to a development's cost reduction, decreasing go-to-market time of the software products.

Moreover, since many FaaS provider have adopted a very small-granularity billing pricing model, usually called \ItalicQuotMark{pay-as-you-go} model, by charging for serverless function execution time rather than for allocated resources, users can save costs. In simple terms, since serverless users pay only while their code executes and, included in the price, FaaS providers take care for several tasks that need to be provided separately in a serverful context, like  provisioning, redundancy, availability, monitoring, logging, and automated scaling, some studies claim that, in practice, customers see cost savings of $4\times$-$10\times$ when moving applications to serverless.

\section{Motivations and Study's Objectives}

Although serverless computing paradigm makes cloud application developing easier guaranteeing a cost-effective solution, there are several obstacles that prevent FaaS platforms to support more general workloads, especially those that must meet strict guarantees.

Scientific literature have already reported a huge amount of limitations preventing a worldwide adoption, among which we recall:

\begin{itemize}
	
	\item the lack of support for those applications having fine-grained state sharing needs, which is primarily due to stateless nature of serverless platforms.
	
	\item the absence of fine-grained coordination capabilities between serverless functions.
	
	\item too simple scheduling policies which, being mainly based on first-come-first-served algorithms, limit how serverless function have to be managed in scenarios such as when incoming demands cannot
	be satisfied by currently available resources.
	
	\item very unpredictable performance due to multiple factors, including
	function placement, \textit{cold starts}, namely the delay incurred due to set-up time required to get a function instance up and running when it is invoked for the first time within a defined period, I/O and network conditions, type of VMs/containers, variability in the hardware and co-location with other functions.
	
	\item security concerns due to fine-grained resource sharing of serverless function which increases the exposure to \textit{side-channel} attacks
	
\end{itemize}

However, according to our point of view, one of the most important obstacles concerns the \textit{quality of service} (\textit{QoS}) levels that should be guaranteed when a serverless application is executed which, as described later, in our study, are quantitatively measured considering cost and response time.

Main difficulties relating to guaranteeing QoS levels are related to several factors, the most important of which is the lack of an adequate performance and cost model, which is crucial to assure support for several constrained applications. Several researchers have already proposed some useful performance and cost models like in \ref{}, \ref{} or \ref{}, but none of them is actually capable to exploit the existing of multiple implementations, that usually exhibit different performances and cost, regarding a same serverless function inside an application. Moreover, no existing solution is able to manage serverless application whose functions are hosted on multiple FaaS platform. Managing multiple function implementation on multiple FaaS platform can be quite useful to guarantee (\textit{QoS}) levels.

Another very important obstacle, extensively studied by various researchers, is the optimization problem regarding serverless function configuration parameters. In fact, as we said previously, when cloud application developers want to run a serverless function, is necessary to specify several configuration's parameters regarding its execution environment, like RAM memory size, time limits or CPU power; unfortunately configuring such parameters correctly, meeting cost and response time constraints, is not trivial. 

Several studies have already shown that aforementioned configuration parameters significantly affect the cost and response time of any serverless functions. \ref{COSE} have shown that serverless function's response time decreases when the RAM memory size, allocated to the function, is increased; however, due to pricing models adopted by the vast majority of FaaS providers, according to which cost depends proportionally on the amount of RAM memory size allocated to a serverless function, a too large value for memory can result in higher costs.

Serverless function's configuration parameters problem is quite more complicated than that because, due to the tight coupling between the pricing model and the amount of allocated resources, even setting a small value of RAM memory size can lead to higher costs, due to a longer execution time of the functions. Moreover, the marginal improvement in response time decreases as the memory increases. Despite several solutions, based on statistical learning, like in \ref{}, or greedy algorithms, as in \ref{}, already exist, none of them seem to consider the changing of execution environment over the time. In fact, besides parameters configuration, serverless functions performance is affected by several other factors like traffic, \textit{cold starts} and network conditions. Sometimes, in order to meet QoS levels, due to changing regarding execution environments on FaaS platform, can be useful to change, at runtime, serverless functions configuration. There is no existing solution capable to dynamically adapt itself when execution environments on FaaS platform changes.

Unfortunately, despite FaaS platforms continuously advance the support for serverless applications, existing solutions are dominated by a few large-scale public providers resulting in mostly non-portable FaaS solutions which exacerbates the provider lock-in problem. There is no agreement on how a serverless application is to be represented among different FaaS providers, therefore shift to another vendor's platform can be difficult and costly, making customers more dependent on a single FaaS solution. We believe that a prospective user must be able to choose the FaaS platform that best suit his/her QoS requirements.

The main goal of our work is to mitigate 



\section{Contributions}

Our work's contributions can be summarized as follows:

\begin{itemize}
	\item Firstly, we developed a formal definition of a serverless application workflow, abstracting sequences of serverless functions, including parallel, branch and loop structures.
	
	\item We formalized a performance model in order to predict a serverless application's performance, in term of economic cost and response time, when the latter is executed with a given ``\textit{configuration}". 
	
	Aforementioned model was been necessary to develop a methodological way to guarantee the service level agreement (SLA) of serverless applications in an economic manner.
	
	\item In order to significantly promote the use of serverless paradigm by applications that must meet strict guarantees, we present a methodology and a software framework capable to find the ``\textit{best configuration}" for a serverless application.
	
	This can be done, providing the suitable configuration of each serverless function within a serverless application by solving an optimization problem, that is a LP problem, derived from our performance model.
	
	\item Generally, owing to he complexity of the problem, its very difficult to find an optimal configuration for a serverless applications that guarantees the respect of users performance constraints. 
	
	This situation can discourage the adoption of serverless paradigm, mostly by users that use real-time applications. 
	
	To provide a predictable and a very rapid system response, we developed a heuristic algorithm capable to find a near optimal solution exploiting a probabilistic technique belonging to colony optimization algorithm (ACO) family.  
	
	\item In order to mitigate the impact of vendor lock-in issue, primarily related to the high migration costs paid by users when they wish change FaaS provider to better meet their needs, our software system exploits an  unique way to represent a serverless application, in such a way that very little changes to that representation code are needed to complete any switching process.
	
	To further reduce vendor lock-in issue, the managing of serverless application 
	
	
	exploiting diff serverless function hosted on any supported provider using identical operations.
	
	\item Since different FaaS platform providers can exhibit different performance imposing different billing methodology, in order to find the best possible configuration of a given serverless application capable to meet users expectations, we developed both our performance model and our software system compatible with a hybrid FaaS platform execution.
	
	In other words, to find the best possible serverless application's configuration, our solution is able to invoke the execution of serverless function hosted on different FaaS providers, exploiting their different characteristics in term of performance or billing system.
		
	\item Since serverless application decouples its business logic into a
	group of serverless functions hosted on FaaS platforms 
	
	We have developed a software system acting as coordinator
	
	 
	To complete the business logic of the application, in-
	teractions among decoupled functions are indispensable.
	In most cases, a coordinator is required to chain together
	components of the application, handle events among func-
	tions, and trigger functions in the correct order defined by
	the business logic.
	
	
	
	
	of provider migrati, owing to high from one provider to another, 
	
	
	
	
		In fact, hiding any difference regarding serverless compound functions representation and the way according to which the latter can be accessed, reaching an agreement on how a compound function is to be represented among different FaaS providers, we allow our users to access to any serverless function hosted on any supported provider using identical operations.
	
	\item We can significantly reduce the impact of vendor lock-in problem. 
	
	Providing an unique way to represent a serverless compound functions, if our final users wish, they will be able to use serverless functions hosted on another FaaS provider without rewriting their serverless compound functions entirely, because very little changes to FC representation code are needed to complete the switching process.
	
	\item To improve access transparency to FaaS platform services, we have developed a middle-ware with following characteristics:

	
	
	
\end{itemize}


The goal of our work is to propose a 



Self-Adaptation (SA) has been widely rec-
ognized [17], [18], [26] as an effective approach to deal
with the increasing complexity, uncertainty and dynamicity
of these systems. A well recognized engineering approach
to realize self-adaptation is by means of a feedback control
loop called MAPE-K [26], [13] and conceived as a sequence
of four computations Monitor-Analyze-Plan-Execute over a
Knowledge base.




The goal of self-adaptation is to alleviate the manage-
ment problem of complex software systems that operate
in highly changing and evolving environments. Such
systems should be able to dynamically adapt themselves
to their environment with little or no human inter-
vention, in order to meet both functional requirements
concerning the overall logic to be implemented and non-
functional requirements concerning the quality of service
(QoS) levels that should be guaranteed.

\chapter{Conceptual Model}

From an architectural point of view, our system software framework acts as a simple client-server model, according to which processes are divided into two groups:

\begin{itemize}
	
	\item fdsfdsfsd
	
	\item dfsfffsdfdsfdsfsdfdfdsfsdfdfsdfdfsdfsd
	
\end{itemize}


In the basic client-server model, processes in a distributed system are divided
into two (possibly overlapping) groups. A server is a process implementing
a specific service, for example, a file system service or a database service. A
client is a process that requests a service from a server by sending it a request
and subsequently waiting for the server’s reply. This client-server interaction,
also known as request-reply behavior 
A framework 




and has the ability to adapt
to changes in the execution time of serverless functions. 



The basic goal of adaptation is to make the system
able to fulfill its functional and/or non functional require-
ments, despite variations in its operating environment,
which are very likely to occur in the SOA domain. As
pointed out in the introduction, our focus in this paper is
on non functional requirements concerning the delivered
QoS and cost. In the SOA domain, these requirements
are usually the result of a negotiation process engaged
between the service provider and user, which culmi-
nates in the definition of a Service Level Agreement (SLA)
concerning their respective obligations and expectations
[39]. In a stochastic setting, a SLA specifies guarantees
about the average value of quality attributes, or more
tough guarantees about the higher moments or percentiles
of these attributes.





From an architectural point of view, our software system uses a top-down approach based on MAPE-K feedback control loop. Re-
searchers wrote that:


This section details the architecture of Sequoia. Sequoia is a
standalone scheduling framework that can be deployed as a
proxy to existing cloud services or directly integrated into
platforms such as OpenWhisk. The framework consists of
three main logical entities (Figure 9): a QoS Scheduler, a Log-
ging Framework, and a Policy Framework. The QoS Scheduler
decides where, when, and how to run specic functions or
function chains. The QoS Scheduler integrates tightly with
the Logging Framework, whose responsibility is to store
real-time metrics that describe the current and historical
state of the serverless environment. Both the QoS Scheduler
and Logging Framework interface with the Policy Frame-
work to make scheduling decisions. All three components
are highlighted below.


Modern software systems typically operate in dynamic
environments and deal with highly changing operational con-
ditions: 

 well recognized engineering approach
to realize self-adaptation is by means of a feedback control
loop called MAPE-K [26], [13] and conceived as a sequence
of four computations Monitor-Analyze-Plan-Execute over a
Knowledge base.


Why. The basic goal of adaptation is to make the system
able to fulfill its functional and/or non functional require-
ments, despite variations in its operating environment,
which are very likely to occur in the SOA domain. As
pointed out in the introduction, our focus in this paper is
on non functional requirements concerning the delivered
QoS and cost. In the SOA domain, these requirements
are usually the result of a negotiation process engaged
between the service provider and user, which culmi-
nates in the definition of a Service Level Agreement (SLA)
concerning their respective obligations and expectations
[39]. In a stochastic setting, a SLA specifies guarantees
about the average value of quality attributes, or more
tough guarantees about the higher moments or percentiles
of these attributes.

\subsection{Communication Semantics}

Since message delivery reliability issues were out of the scope our this work, we have adopt the so-called maybe comunication semantics

Maybe semantics: With maybe semantics, the remote procedure call may be executed
once or not at all. Maybe semantics arises when no fault-tolerance measures are applied
and can suffer from the following types of failure:
• omission failures if the request or result message is lost;
• crash failures when the server containing the remote operation fails.
If the result message has not been received after a timeout and there are no retries, it is
uncertain whether the procedure has been executed. If the request message was lost, then
the procedure will not have been executed. On the other hand, the procedure may have
been executed and the result message lost. A crash failure may occur either before or
after the procedure is executed. Moreover, in an asynchronous system, the result of
executing the procedure may arrive after the timeout. Maybe semantics is useful only for
applications in which occasional failed calls are acceptable.


\subsection{title}

When building a distributed system out of existing components, we immedi-
ately bump into a fundamental problem: the interfaces offered by the legacy
component are most likely not suitable for all applications. In Section 1.3 we
discussed how enterprise application integration could be established through
middleware as a communication facilitator, but there we still implicitly as-
sumed that, in the end, components could be accessed through their native
interfaces.


\section{Software system architecture}

From an architectural point of view, our software system acts as a standalone framework QoS-driven runtime adaptation for serverless applications.



 that implements it, for QoS-
driven runtime adaptation 


This section details the architecture of Sequoia. Sequoia is a
standalone scheduling framework that can be deployed as a
proxy to existing cloud services or directly integrated into
platforms such as OpenWhisk. The framework consists of
three main logical entities (Figure 9): a QoS Scheduler, a Log-
ging Framework, and a Policy Framework. The QoS Scheduler
decides where, when, and how to run specic functions or
function chains. The QoS Scheduler integrates tightly with
the Logging Framework, whose responsibility is to store
real-time metrics that describe the current and historical
state of the serverless environment. Both the QoS Scheduler
and Logging Framework interface with the Policy Frame-
work to make scheduling decisions. All three components
are highlighted below.






\subsection{Resource owner}

A \textit{resource owner}, henceforward denoted with $R$, represents an entity capable of \textit{creating}, \textit{modifying} and \textit{authorizing} access to several resources of our system.

\newpage

\chapter{Model's formalization}

\section{SLA's definition}

As said previously, our system's goal is the fulfillment of both \textit{functional requirements}, concerning the orchestration of a serverless workflow, and \textit{non-functional} requirements, concerning the \textit{quality of service} (QoS) levels that should be guaranteed.

In our context, aforementioned non-functional requirements are the result of a commitment, signed by both our software system and a customer, which lead  up to the definition of a \textit{Service Level Agreement} (SLA) specifying guarantees about the \textit{average values} of the following attributes:

\begin{description}
	\item[Response time] the interval of time elapsed from the
	serverless application invocation to its completion;
	\item[Cost] the price charged for the execution of all serverless function belonging to an application;
\end{description}

Is very important to emphasize that our framework considers SLAs whose conditions should be hold at \textit{local level}, that is focusing only on the fulfillment of all requirements regarding a \textit{single} serverless application invocation's request, which is forwarded by only one customer. Consequently, since the adaptation actions regards a single request, the framework acts irrespectively of whether aforementioned request belongs to some flow generated by one or more customers.

Formally, according to our model, for each serverless application invocation, a SLA is defined as follows:

\begin{equation}
	SLA \mathDef \left\langle (RT,w_{RT}),(C,w_{C}) \right\rangle 
\end{equation}
   
where:

\begin{itemize}
	\item $RT \in \mathbb{R}^+$ is the upper bound on the average
	response time of the serverless application.
	
	\item $C \in \mathbb{R}^+$ is the upper bound on average service cost per serverless application invocation.
	
	\item $w_{RT}, w_{C} \in \Rplus \cap \left[ 0,1 \right]$ represent, respectively, the SLA attributes weights regarding the response time and the cost; simply, greater a SLA attribute weight value, the greater is the importance of that attribute. Weights for the different SLAs attributes are used to improve our flexibility regarding meeting customer's expectations. 
	
	Formally, following constraint must be hold:
	
	\begin{equation}
		w_{RT} + w_{C} = 1
	\end{equation}

	We will describe later how weights for the different SLA's attributes can be used.
\end{itemize}

\section{Serverless choreography}

According to our model, a \textit{serverless choreography}, or simply \textit{choreography}, represents the \textit{resource} used to model both the concepts of serverless function and serverless application (or compound serverless function).

Informally, that abstraction has been derived from that of a \textit{control-flow graph} which, as known, describes, using graphs notation, all paths that might be traversed through a serverless application during its execution \ref{}. Similarly, a choreography describes calling relationships between functions belonging to an application in a serverless environment, combining them using several types of control-flow structures, like sequence, branch, loop or connectors for a parallel execution. 

\subsection{Preliminary definitions}

In order to provide a formal definition of a choreography, we have to define some very useful notations. 

Let $n \in \N$ and $G = (\Phi,E)$ a directed graph where:

\begin{itemize}
	\item $\Phi$ is a finite set of vertices, such that $|\Phi| = n$;
	\item  $E \subseteq \Phi \times \Phi $ is a finite set of ordered pairs of vertices $e_{ij} = \left( \phi_i, \phi_j \right)$, where $\phi_i \in \Phi$ to $\phi_j \in \Phi$ for any $i,j \in \N \cap \left[ 1, n \right]$. Any ordered pair of vertices is also called \textit{directed edge};
\end{itemize}

Then, we will adopt following notations:

\begin{itemize}
	\item A \textit{path} of $G$ is defined as a finite sequence of distinct vertices and edges. We will denote a path by $\pi$, which formally can be represented as follows:
	
	\begin{equation}
		\pi = \phi_1 e_1 \phi_2 e_2 \ldots e_{n-2}\phi_{n-1} e_{n-1} \phi_n
	\end{equation}
	
	where:
	
	\begin{itemize}
		\item $\phi_i \in \Phi$, for all $i \in \N \cap \left[ 1, n \right]$
		\item $e_i = \left( \phi_i, \phi_{i+1} \right) \in E$, for all $i \in \N \cap \left[ 1, n-1 \right]$
	\end{itemize}
	
	\item Let $\phi_i,\phi_j \in \Phi$ for any $i,j \in \N \cap \left[ 1, n \right]$, the set denoted by $\Pi(\phi_i, \phi_j)$ identifies all possible paths starting from vertex $\phi_i$ and ending at vertex $\phi_j$.
	
	\item For any $u \in \N \cap \left[ 1, n \right]$, the set $out(\phi_u)$ ($in(\phi_u)$) denotes all edges starting (ending) from (to) vertex $\phi_u$, while the set $succ(\phi_u)$ ($pred(\phi_u)$) includes all direct successor (predecessors) vertices of $\phi_u$. Formally:
	
	\begin{eqnarray}\label{outDef}
		out(\phi_u) & \mathDef & \left\lbrace (\phi_u, \phi) \in E, \quad \forall \phi \in \Phi  \right\rbrace \\
		in(\phi_u) & \mathDef & \left\lbrace (\phi, \phi_u) \in E, \quad \forall \phi \in \Phi  \right\rbrace \\
		succ(\phi_u) & \mathDef & \left\lbrace \phi \in \Phi \mid (\phi_u, \phi) \in out(\phi_u)  \right\rbrace \\
		pred(\phi_u) & \mathDef & \left\lbrace \phi \in \Phi \mid (\phi, \phi_u) \in in(\phi_u)  \right\rbrace 
	\end{eqnarray}
\end{itemize}

\subsection{Serverless choreography definition}

According to serverless paradigm, the execution of an application always starts with a particular function, usually triggered through events or HTTP requests, acting as ``\textit{entry point}" of the serverless workflow; any other functions, belonging to the application, will be invoked subsequently according to the business logic specified by customer. Conversely, the execution of an application ends when the execution of its last function ends, which acts as the ``\textit{end point}" of the serverless workflow. 

Assuming that any serverless application has only one function acting as entry point, let $n \in \N \SetMinusZero$ and $R$ a resource owner, then a choreography, owned by $R$, is a weakly connected\footnote{A directed graph is called \textit{weakly connected} if replacing all of its directed edges with undirected edges produces a connected undirected graph.} weighted directed graph denoted as follows:

\begin{equation}
	\mathcal{C}_R \mathDef (\Phi,E)
\end{equation}

where:

\begin{itemize}
				
	\item Each vertex $\phi \in \Phi$, where $|\Phi| = n$, is called \textit{abstract serverless function}, or simply \textit{abstract  function}, and it represents a generic computational unit. :
	
	\begin{itemize}
				
		\item $P_{exit} : \Phi \times \Rplus \to \Rplus \cap \SetFromZeroToOne$ represents the so-called \textit{exit probability function}, according to which $P_{exit}(\phi,t)$ denotes the probability of execution's termination of $\mathcal{C}_R$, when the execution of $\phi$ is terminated, at time $t$.
	\end{itemize}
	
	\item Let $i,j \in \N \cap \left[ 1, n \right]$ and $\phi_i, \phi_j \in \Phi$, any directed edge $e_{ij} = \left( \phi_i, \phi_j \right) \in E$ represents the calling relationship between two abstract functions, which depends on the business logic defined by $R$. 
	
	In our context, any directed edge $\left( \phi_i, \phi_j \right) \in E$ states that the abstract function $\phi_j$ \textit{can} be called by $\phi_i$;
	
	\item Let $i,j \in \N \cap \left[ 1, n \right]$, the number $p_{ij} \in \Rplus \cap \SetFromZeroToOne$ is the weight assigned to the edge $\left(\phi_i, \phi_j \right) \in E$, where: 
	
	\begin{itemize}
		
		\item The number $p_{ij}$ represents the so-called \textit{transition probability} from $\phi_i$ to $\phi_j$, that is the probability of invoking $\phi_j$ when the execution of $\phi_i$ is terminated;
		
		\item $P : \Phi \times \Phi \times \Rplus \to \Rplus \cap \left[ 0, 1 \right]$, called \textit{transition probability function}, is such that $P\left(\phi_i, \phi_j, t \right) = p_{ij}$, where $t$ is the time. Obviously, $P\left(\phi_i, \phi_j, t \right) = 0$ implies that $\phi_i$ cannot invoke $\phi_j$ but it does not imply that the directed edge $\left( \phi_i, \phi_j \right) \notin E$; directed edges having null weight can belong to $E$.
		
		\item For any path $\pi = \phi_1 e_1 \ldots e_{n-1} \phi_n$ of $\mathcal{C}_R$, we define \textit{transition probability of the path $\pi$} the following quantity:
		
		\begin{equation}
			TPP(\pi, t) = \prod_{i = 1}^{n-1} P\left(\phi_i, \phi_{i+1}, t \right)
		\end{equation}
	
		Particularly, an abstract function $\phi \in \Phi$ is said \textit{unreachable} if, for any time $t$, following condition is hold:
	
		\begin{equation}\label{cond3}
			\displaystyle \sum_{\pi \in \Pi(\alpha(\mathcal{C}_R), \phi)} TPP(\pi, t) = 0
		\end{equation}
	
		Conversely, a $\phi \in \Phi$ is said \textit{reachable} when:

		\begin{equation}\label{cond3}
			\displaystyle \sum_{\pi \in \Pi(\alpha(\mathcal{C}_R), \phi)} TPP(\pi, t) > 0
		\end{equation}
		
	\end{itemize}

	\item $\Phi$ must be such that following condition are hold:
	
	\begin{eqnarray}
		\exists !  \phi \in \Phi &\mid & in(\phi) = \emptyset \label{cond1} \\
		\exists !  \phi \in \Phi & \mid & out(\phi) = \emptyset \label{cond2}
	\end{eqnarray}
	
	that is, $\Phi$ has to contain only one abstract function acting as entry point and only one other acting as end point.	In other terms, we can state that:
	
	\begin{equation}
		\begin{array}{lcll}
			\phi \in \Phi & \text{ is the \textit{entry point} of } \mathcal{C_R} & \Leftrightarrow & in(\phi) = \emptyset \\
			\phi \in \Phi & \text{ is an \textit{end point} of } \mathcal{C_R} & \Leftrightarrow & out(\phi) = \emptyset
		\end{array}
	\end{equation}
	
	Finally, we will use the notation $\alpha(\mathcal{C}_R)$ to denote the vertex $\phi \in \Phi$ acting as entry point of the choreography $\mathcal{C}_R$; conversely, we will adopt the notation $\omega(\mathcal{C}_R)$ to denote the end point.
	
	
	\item Following condition must be hold:
	
	\begin{equation}\label{cond3}
		|\Pi(\alpha(\mathcal{C}_R), \phi)| \geq 1 \qquad \forall \phi \in \Phi
	\end{equation}

	in other words, at least one path starting from the entry point of $\mathcal{C}_R$ to each abstract function $\phi \in \Phi$ must to exist. 
	
\end{itemize} 

A choreography $\mathcal{C}_R$ can be uniquely identified by an ordered pair $(a, b)$, where $a$ is the name of the resource owner $R$, while $b$ is the function choreography name. 

We will say that a choreography models a serverless function when $|\Phi| = 1$ and $|E| = 0$; conversely, it models a serverless application when $|\Phi| > 1$ and $|E| > 0$.

From now, a choreography $\mathcal{C}_R$ will be briefly denoted by $\mathcal{C}$ when no confusion can arise about the resource owner $R$.

\subsubsection{Abstract Serverless Function}

Supposing that a choreography $\mathcal{C} = (\Phi,E)$ is given.

As said previously, any $\phi \in \Phi$ represents an abstract function, which is a \textit{resource} modeling a computational unit required by business logic provided by developers.

According to our model, there are two types of abstract functions implementations:

\begin{itemize}
	
	\item $\phi$ is called \textit{serverless executable functions}, or simply \textit{executable function}, when $\phi$ contains \textit{executable code}; therefore, any executable function naturally models a serverless function.
	
	$\mathscr{F_E}(\mathcal{C})$ is defined as the set containing all executable function of $\mathcal{C}$ which is formally defined as follows:
	
	\begin{equation}
		\mathscr{F_E(\mathcal{C})} \mathDef \left\lbrace \phi \in \Phi \mid \phi \text{ is a serverless executable function }\right\rbrace 
	\end{equation}
	
	However, multiple different implementations of a given executable function can be provided by developers which, although they must be semantically and logically equivalent, may eventually expose different performance or cost behavior. We call these different implementations as \textit{concrete serverless function}, or simply, \textit{concrete function} of $\phi$.
	
	For any $\phi \in \mathscr{F_E}(\mathcal{C})$, we will use $\textbf{F}_{\phi}$ notation to represent the so-called \textit{implementation-set} of $\phi$, that is the set containing all concrete function implementing $\phi$, which are denoting using $f_{\phi}$ notation. 

	Later, we will explain how our framework is able to pick, for all $\phi \in \mathscr{F_E}(\mathcal{C})$, exactly one $f_{\phi} \in \textbf{F}_{\phi}$ whose properties allow us to meet user-specified QoS requirements.
	
	Finally, according to our model point of view, following conditions must be hold: 
	
	\begin{eqnarray}
		|succ(\phi)| & = & n \qquad \forall \phi \in \mathscr{F_E}(\mathcal{C}) \label{eq:executableFP1} \wedge n \in \left\{0,1\right\} \\
		P(\phi, \phi_x,t) & = & 1 \qquad \forall \phi, \phi_x \in \mathscr{F_E}(\mathcal{C}) : \phi_x \in succ(\phi) \wedge |succ(\phi)| = 1, \forall t \qquad \qquad \label{eq:executableFP2} \\
		P_{exit}(\phi, t) & = & 0 \qquad \forall \phi \in \mathscr{F_E}(\mathcal{C}), \forall t
	\end{eqnarray}


	\item $\phi$ is called \textit{serverless orchestration functions}, or simply \textit{orchestration functions}, when $\phi$ contains \textit{orchestration code}. 
	
	According to our model, orchestration code represents the logic required to chain together any components of an application, evaluate branch and loop conditions, handling events and triggering executable functions in the correct order according to the business logic. In other words, orchestration code is used to manage the control-flow of any application.
	
	$\mathscr{F_O}(\mathcal{C})$ is defined as the set containing all orchestration functions of $\mathcal{C}$ and it is formally defined as follows:
	
	\begin{equation}
		\mathscr{F_O}(\mathcal{C}) \mathDef \left\lbrace \phi \in \Phi \mid \phi \text{ is a serverless orchestration function }\right\rbrace 
	\end{equation}

	Moreover, each orchestration function $\phi \in \mathscr{F_O}(\mathcal{C})$ has a \textit{type} which, as we will explain later, determines how choreography's performances are computed. The orchestration function's type can be determined based on certain characteristics of the graph of $\mathcal{C}$. According to our model, we distinguish four orchestration functions types exist: \textit{Branch}, \textit{Conditional Loop}, \textit{Parallel} and \textit{Exit}. We will give a better explanation about them very soon.

	Moreover, according to our model, orchestration functions travel always in pairs. Each $\phi \in \mathscr{F_O}(\mathcal{C})$ can be either an \textit{opening} or a \textit{closing} orchestration function of a given type: the first indicates the beginning of particular section of $\mathcal{C}$, called \textit{structure}, while the second denotes its  end. Anyway, if $\tau$ denotes the orchestration function's type, we will use $\tau_{\alpha}$ and $\tau_{\omega}$ notations to denote, respectively, an opening and a closing orchestration function of type $\tau$. Moreover, we will use the notation $\mathscr{T}(\phi)$ to represent the function returning the orchestration function type of $\phi$. We will give more details about this very soon.
	
	Finally, following condition must be hold:
	
	\begin{eqnarray}
		|succ(\phi)| & = & n \qquad \forall \phi \in \mathscr{F_O}(\mathcal{C}) : \mathscr{T}(\phi) = \tau_{\omega} \wedge n \in \left\{0,1\right\} \\		
		P(\phi, \phi_x,t) & = & 1 \qquad \forall \phi, \phi_x \in \mathscr{F_O}(\mathcal{C}) : \\		
		& & \qquad \qquad \mathscr{T}(\phi) = \tau_{\omega} \wedge \phi_x \in succ(\phi) \wedge |succ(\phi)| = 1, \forall t \qquad \qquad  \nonumber
	\end{eqnarray}
	
\end{itemize}

Clearly, based on above definitions, we can say: 

\begin{eqnarray}
	\mathscr{F_E}(\mathcal{C}) \cap \mathscr{F_O}(\mathcal{C}) & = & \emptyset \\
	\mathscr{F_E}(\mathcal{C}) \cup \mathscr{F_O}(\mathcal{C}) & = & \Phi \\
	|\mathscr{F_E}(\mathcal{C})| + |\mathscr{F_O}(\mathcal{C})| &=& |\Phi| 
\end{eqnarray}

Any abstract function $\phi$ is uniquely identified by an ordered pair $(a, b)$, where:
\begin{itemize}
	\item $a$ represents the identifier of the choreography $\mathcal{C}$;
	\item $b$ is the name of the abstract serverless function $\phi$;
\end{itemize}

\subsubsection{Executability condition}

Unfortunately, our system software has been not designed to manage serverless concrete functions hosted on FaaS platform providers. In other words, is not actually possible to execute CRUD (\textit{Create}, \textit{Read}, \textit{Update}, and \textit{Delete}) operations regarding concrete functions on FaaS platform providers.

Therefore, let $\mathcal{C} = (\Phi,E)$ a choreography, we assume always that all concrete functions $f_{\phi} \in \textbf{F}_{\phi}$, for all $\phi \in \mathscr{F_E}(\mathcal{C})$, are already deployed on one or more FaaS platform by customers. Then, in order to effectively start the execution of $\mathcal{C}$, is required that, for each executable function $\phi \in \mathscr{F_E}(\mathcal{C})$, \textit{at least one} implementation exists.

Formally, we said that a choreography is \textit{executable} when: 

\begin{eqnarray}
	\label{eqn:SchedulabilityConditionOne}
	\mathcal{C} \text{ is executable } & \Leftrightarrow & |\textbf{F}_{\phi}| \geq 1 \qquad \forall \phi \in \mathscr{F_E}(\mathcal{C})
\end{eqnarray}

We will only deal with executable choreographies.

\subsubsection{Serverless sub-choreography}

Let $\mathcal{C} = (\Phi,E)$ a choreography, the weakly connected weighted directed sub-graph $\mathcal{C}^*$ of $\mathcal{C}$, defined as follows:

\begin{equation}
	\mathcal{C}^* \mathDef (\Phi^*,E^*) \qquad \text{ where } \Phi^* \subseteq \Phi \wedge E^* \subseteq E
\end{equation}

is called \textit{serverless sub-choreography} of $\mathcal{C}$, or simply \textit{sub-choreography} of $\mathcal{C}$, when the conditions \ref{cond1}, \ref{cond2} and \ref{cond3} are hold.

\subsubsection{Serverless pipeline choreography}\label{PipelineDefinitionSection}

Suppose to have a choreography $\mathcal{C} = (\Phi,E)$ satisfying following conditions:

\begin{eqnarray}
	|in(\phi)| & = & 1 \qquad \forall \phi \in \Phi \setminus \left\{ \alpha(\mathcal{C}) \right\} \label{pipeline1} \\
	|out(\phi)| & = & 1 \qquad \forall \phi \in \Phi \setminus \left\{ \omega(\mathcal{C}) \right\} \label{pipeline2} \\
	P(\phi_x, \phi_y, t) & = & 1 \qquad \forall x,y \in \SetFromOneTo{|\Phi|}, \forall t \label{pipeline3}
\end{eqnarray}

Any choreography $\mathcal{C}$ satisfying \ref{pipeline1}, \ref{pipeline2} and \ref{pipeline3} will be called \textit{serverless pipeline choreography}, or simply a \textit{pipeline choreography}.

\subsection{Serverless choreography's structure}

Let $\mathcal{C} = (\Phi,E)$ a choreography. 

Informally, a \textit{structure} is defined as any sub-choreography $\mathcal{C}^*= (\Phi^*,E^*,\Theta_{\mathcal{C}^*})$ of $\mathcal{C}$ whose entry point $\alpha(\mathcal{C}^*)$ and the end point $\omega(\mathcal{C}^*)$ are, respectively, opening and closing orchestration functions having same type $\tau$ possibly coinciding. Formally:

\begin{equation}
	\mathcal{C}^* \text{ is a structure } \Leftrightarrow \left\{ \begin{array}{l}
		\alpha(\mathcal{C}^*), \omega(\mathcal{C}^*) \in \mathscr{F_O}(\mathcal{C}^*) \\
		
		\mathscr{T}(\alpha(\mathcal{C}^*)) = \tau_{\alpha} \\ \mathscr{T}(\omega(\mathcal{C}^*)) = \tau_{\omega}
	\end{array} \right.
\end{equation}

For the aim of our work, the most important aspect is that every structure can be viewed as a \ItalicQuotMark{set} of sub-choreographies. Formally, let $c \in \N \setminus \left\{0\right\}$ and $\mathcal{C}^{**} = (\Phi^{**},E^{**})$ a sub-choreography of $\mathcal{C}^*$ such that:

\begin{equation}
	\begin{array}{lll}
		\Phi^{**} & \mathDef & \Phi^* \setminus \left\lbrace \alpha(\mathcal{C}^*),\omega(\mathcal{C}^*) \right\rbrace   \\
		E^{**} & \mathDef & E^* \setminus \Big[ out \Big( \alpha(\mathcal{C}^*) \Big) \cup in \Big( \omega(\mathcal{C}^*) \Big) \Big]
	\end{array}
\end{equation}

that is, $\mathcal{C}^{**}$ is obtained removing both the entry point and end point of $\mathcal{C}^{*}$, including any edges starting/ending from/to them. Then, $\Theta_{\mathcal{C}^*}$, such that $|\Theta_{\mathcal{C}^*}| = c$ denotes the set containing all connected components of $\mathcal{C}^{**}$ satisfying \ref{cond1}, \ref{cond2} and \ref{cond3} conditions; in other words, each connected component of $\mathcal{C}^{**}$ represents a sub-choreography of $\mathcal{C}$. Formally:

\begin{eqnarray}
	\Theta_{\mathcal{C}^*} & \mathDef & \left\lbrace \theta_1, \ldots ,\theta_c \right\rbrace \\ & = & \bigcup_{i = 1}^c \left\{ \begin{array}{l}
		\theta_i = (\Phi^{**}_i, E^{**}_i) : \Phi^{**}_i \subset \Phi^{**} \wedge E^{**}_i \subset E^{**} \\ 
		\Phi^{**}_i \cap \Phi^{**}_j = E^{**}_i \cap E^{**}_j = \emptyset \qquad j \in \SetFromOneTo{c}: j \neq i \\
		\text{$\theta_i$ is a connected component of $\mathcal{C}^{**}$} \\	
		\text{$\theta_i$ satisfies \ref{cond1}, \ref{cond2} and \ref{cond3} conditions}
	\end{array} \right\} \nonumber
\end{eqnarray}

Sometimes, to reach our objective, will be necessary to \ItalicQuotMark{replace} structure inside a choreography in order to convert it into a pipeline type, performing the so-called \textit{workflow simplification process}, according to which any structure can be replaced by a single orchestration function $\phi_{fake}$, obtaining a new choreography $\mathcal{C}' = (\Phi',E')$, such that:

\begin{eqnarray}
	\Phi' & = & \left\{ \Phi \setminus \Phi^* \right\} \cup \left\{ \phi_{fake} \right\} \\
	E' & = & \left\{ E \setminus E^* \right\} \cup \left\{\bigg(prev(\alpha(\mathcal{C}^*)), \phi_{fake}\bigg) \right\} \cup \left\{\bigg(\phi_{fake}, succ(\omega(\mathcal{C}^*)) \bigg) \right\}
\end{eqnarray}

where $\phi_{fake} \in \mathscr{F_O}(\mathcal{C}^*)$ is such that:

\begin{eqnarray}
	RT_{\phi_{fake}}(\textbf{x}_{\mathcal{C}}, t) & = & RT(\mathcal{C}^*,\textbf{x}_{\mathcal{C}}, t) \\
	C_{\phi_{fake}}(\textbf{x}_{\mathcal{C}}, t) & = & C(\mathcal{C}^*,\textbf{x}_{\mathcal{C}}, t) \\
	P_{exit}(\phi_{fake}, t) & = & P_{exit}(\mathcal{C}^*, t)
\end{eqnarray}


\subsubsection{Parallel}

Any structure $\mathcal{P} = (\Phi',E',\Theta)$ such that:

\begin{eqnarray}
	\mathscr{T}(\alpha(\mathcal{P})) = \mathscr{T}(\phi_x) = \tau_{\alpha} \\ \mathscr{T}(\omega(\mathcal{P})) = \mathscr{T}(\phi_y) = \tau_{\omega} \\
	TPP(\pi, t) = 1 & \qquad \forall \pi \in \Pi(\phi_{x}, \phi_{y}), \forall t \\
	|\Theta| = n  & \qquad n \in \N \setminus \left\{0\right\} \\
	E[I_{\theta}] = 1 & \qquad \forall \theta \in \Theta \\
	P_{exit}(\theta, t) = 0 & \qquad \forall \theta \in \Theta \\
	|succ(\alpha(\mathcal{P}))| = |prev(\omega(\mathcal{P}))|
\end{eqnarray}

is called \textit{parallel structure} and $\tau$ denotes a \textit{parallel} type.

\begin{figure}
	\centering
	\begin{tikzpicture}
		
		
		\node[circle,draw=white,minimum width = 1cm] 
		(start) at (-2,0) {};
		
		\node[circle,draw=white,minimum width = 1cm] 
		(end) at (12,0) {};
		
		\node[circle, draw, minimum width = 1cm]        					   
		(branchStartNode) at (0,0) {$\phi_x$};
		
		\node[circle, draw, minimum width = 1cm]        					   
		(branchEndNode) at (10,0) {$\phi_y$};
		
		
		\node[circle, draw, minimum width = 1cm]        					   
		(Node1) at (3.8,2) {$\phi_a$};
		
		\node[circle, minimum width = 0.3cm]       					   
		(Node2) at (5,2) {...};
		
		\node[circle, draw, minimum width = 1cm]       					   
		(Node3) at (6.2,2) {$\phi_b$};
		
		\node[circle, draw, minimum width = 1cm]       					   
		(Node4) at (3.8,-2) {$\phi_c$};
		
		\node[circle, minimum width = 0.3cm]       					   
		(Node5) at (5,-2) {...};
		
		\node[circle, draw, minimum width = 1cm]       					   
		(Node6) at (6.2,-2) {$\phi_d$};
		
		\draw[ultra thick,->] (branchStartNode) edge node[xshift=-30 ,yshift=20]{$P(\phi_x,\phi_a, t) = 1$} (Node1);
		
		\draw[ultra thick,->] (branchStartNode) edge node[xshift=-30 ,yshift=-20]{$P(\phi_x,\phi_c, t) = 1$} (Node4);
		
		\draw[ultra thick,->] (Node3) edge node[xshift=15 ,yshift=20]{$P(\phi_b,\phi_y, t) = 1$} (branchEndNode);
		
		\draw[ultra thick,->] (Node6) edge node[xshift=15 ,yshift=-20]{$P(\phi_d,\phi_y, t) = 1$} (branchEndNode);
		
		% Dashed lines
		
		\draw[dashed,ultra thick, ->] (start) -- (branchStartNode);
		
		\draw[ultra thick, ->] (branchEndNode) -- (end);
		
		\draw[ultra thick, ->] (Node1) -- (Node2);
		
		\draw[ultra thick, ->] (Node2) -- (Node3);
		
		\draw[ultra thick, ->] (Node4) -- (Node5);
		
		\draw[ultra thick, ->] (Node5) -- (Node6);
		
		% Frames
		
		\node[rectangle, draw, dashed, minimum width =3.7cm, minimum height=1.8cm] (sub1) at (5,2) {};
		\node[circle,above of=sub1, yshift=10] (label1) {$\theta_1$};
		
		\node[rectangle, draw, dashed, minimum width =3.7cm, minimum height=1.8cm] (sub2) at (5,-2) {};
		\node[circle,below of=sub2, yshift=-10] (label2) {$\theta_2$};
		
	\end{tikzpicture}
	\caption{Parallel structure in a serverless workflow.}
\end{figure}

Clearly, the response time of a parallel structure is equal to the longest response time of all its sub-choreographies.

Formally, let $\theta_i \in \Theta$ the $i$-th sub-choreography of $\mathcal{P}$, where $i \in \N \cap \left[1,n\right]$, and $\textbf{x}_{\mathcal{C}} \in \textbf{X}_{\mathcal{C}}$ a choreography configuration.

At any time $t$, the average response time and billed cost of $\mathcal{P}$ can be computed as follows:

\begin{equation}
	RT(\mathcal{P},\textbf{x}_{\mathcal{C}}, t) \mathDef max \left\lbrace RT(\theta_i,\textbf{x}_{\mathcal{C}}, t) \mid \theta \in \Theta \right\rbrace 
\end{equation}

\begin{equation}
	C(\mathcal{P},\textbf{x}_{\mathcal{C}}, t) \mathDef \sum_{i = 1}^n C(\theta_i,\textbf{x}_{\mathcal{C}}, t)
\end{equation}

Since, by definition, we imposed that all sub-choreography of $\mathcal{P}$ have exit probability equal to zero, the exit probability of the a parallel structure is zero too. Therefore, formally:

\begin{equation}
	P_{exit}(\mathcal{P}, t) = 0
\end{equation}

\subsubsection{Branch}

Any structure $\mathcal{B} = (\Phi',E',\Theta)$ such that:

\begin{eqnarray}
	\mathscr{T}(\alpha(\mathcal{B})) = \mathscr{T}(\phi_x) = \tau_{\alpha} \\ \mathscr{T}(\omega(\mathcal{B})) = \mathscr{T}(\phi_y) = \tau_{\omega} \\
	|\Theta| = n  & \qquad n \in \N \setminus \left\{0\right\} \\
	P(\phi_x, \alpha(\theta_i),t) \leq 1 & \qquad \forall i \in \SetFromOneTo{n} \\
	\sum_{i = 1}^n P(\phi_x, \alpha(\theta_i),t) = 1 & \\
	E[I_{\theta}] = 1 & \qquad \forall \theta \in \Theta \\
	|succ(\alpha(\mathcal{B}))| = |prev(\omega(\mathcal{B}))|
\end{eqnarray}

is called \textit{branch structure} and $\tau$ denotes a \textit{branch} type. More precisely, if $|\Theta| = 2$, $\mathcal{B}$ will be called \textit{if-else-branch structure}; if $|\Theta| = 2$, $\mathcal{B}$ will be called \textit{switch-branch structure}. When $|\Theta| = 1$ and $(\phi_x,\phi_y) \in E$, $\mathcal{B}$ is called \textit{if-branch structure}. Clearly, it is used to model \texttt{if}, \texttt{if-else} and \texttt{switch} programming structures inside a serverless application. 

\begin{figure}
	\centering
	\begin{tikzpicture}
		
		
		\node[circle,draw=white,minimum width = 1cm] 
		(start) at (-2,0) {};
		
		\node[circle,draw=white,minimum width = 1cm] 
		(end) at (12,0) {};
		
		\node[circle, draw, minimum width = 1cm]        					   
		(branchStartNode) at (0,0) {$\phi_x$};
		
		\node[circle, draw, minimum width = 1cm]        					   
		(branchEndNode) at (10,0) {$\phi_y$};
		
		
		\node[circle, draw, minimum width = 1cm]        					   
		(Node1) at (3.8,2) {$\phi_a$};
		
		\node[circle, minimum width = 0.3cm]       					   
		(Node2) at (5,2) {...};
		
		\node[circle, draw, minimum width = 1cm]       					   
		(Node3) at (6.2,2) {$\phi_b$};
		
		\node[circle, draw, minimum width = 1cm]       					   
		(Node4) at (3.8,-2) {$\phi_c$};
		
		\node[circle, minimum width = 0.3cm]       					   
		(Node5) at (5,-2) {...};
		
		\node[circle, draw, minimum width = 1cm]       					   
		(Node6) at (6.2,-2) {$\phi_d$};
		
		\draw[ultra thick,->] (branchStartNode) edge node[xshift=-30 ,yshift=20]{$P(\phi_x,\phi_a, t) = x$} (Node1);
		
		\draw[ultra thick,->] (branchStartNode) edge node[xshift=-30 ,yshift=-20]{$P(\phi_x,\phi_c, t) = 1-x$} (Node4);
		
		\draw[ultra thick,->] (Node3) edge (branchEndNode);
		
		\draw[ultra thick,->] (Node6) edge (branchEndNode);
		
		% Dashed lines
		
		\draw[dashed,ultra thick, ->] (start) -- (branchStartNode);
		
		\draw[ultra thick, ->] (branchEndNode) -- (end);
		
		\draw[ultra thick, ->] (Node1) -- (Node2);
		
		\draw[ultra thick, ->] (Node2) -- (Node3);
		
		\draw[ultra thick, ->] (Node4) -- (Node5);
		
		\draw[ultra thick, ->] (Node5) -- (Node6);
		
		% Frames
		
		\node[rectangle, draw, dashed, minimum width =3.7cm, minimum height=1.8cm] (sub1) at (5,2) {};
		\node[circle,above of=sub1, yshift=10] (label1) {$\theta_1$};
		
		\node[rectangle, draw, dashed, minimum width =3.7cm, minimum height=1.8cm] (sub2) at (5,-2) {};
		\node[circle,below of=sub2, yshift=-10] (label2) {$\theta_2$};
		
		legend
		
		\matrix [below left] at (current bounding box.north east) {
			\node [] {$x \in \mathbb{R} : 0 \leq x \leq 1$}; \\
		};
		
		
	\end{tikzpicture}
	\caption{An if-else-branch structure in a serverless workflow}
\end{figure}

Since any branch have the same possibility of be traversed during different execution of $\mathcal{B}$, we have adopted naive probability to model transition probabilities. Formally:

\begin{equation}\label{naiveProbability}
	P(\phi_x, \alpha(\theta),t) = \dfrac{|I_{\theta}(t)|}{|I_{\mathcal{B}}(t)|} \qquad \forall \theta \in \Theta
\end{equation}

where $|I_{\theta}|$ ($|I_{\mathcal{B}}|$) denotes how many times the sub-choreography $\theta$ (structure $\mathcal{B}$) was been executed in the past at time $t$.

Let $\theta_i \in \Theta$ the $i$-th sub-choreography of $\mathcal{B}$, where $i \in \N \cap \left[1,n\right]$. At any time $t$, the response time and the cost of the branch structure $\mathcal{B}$ can be computed as follows:

\begin{equation}
	RT(\mathcal{B},\textbf{x}_{\mathcal{C}}, t) \mathDef \sum_{i = 1}^n P(\phi_{x}, \alpha(\theta_i),t) \cdot  RT(\theta_i,\textbf{x}_{\mathcal{C}}, t)
\end{equation}

\begin{equation}
	C(\mathcal{B},\textbf{x}_{\mathcal{C}}, t) \mathDef \sum_{i = 1}^n P(\phi_{x}, \alpha(\theta_i),t) \cdot C(\theta_i,\textbf{x}_{\mathcal{C}}, t)
\end{equation}

The exit probability of a branch structure can be computed as follows:

\begin{equation}
	P_{exit}(\mathcal{B}, t) \mathDef \sum_{i = 1}^n P(\phi_{x}, \alpha(\theta_i),t) \cdot P_{exit}(\theta_i, t)
\end{equation}

\subsubsection{Conditional loop}

Any structure $\mathcal{L} = (\Phi',E',\Theta)$ such that:

\begin{eqnarray}
	\mathscr{T}(\alpha(\mathcal{L})) = \mathscr{T}(\phi_x) = \tau_{\alpha} \\ \mathscr{T}(\omega(\mathcal{L})) = \mathscr{T}(\phi_y) = \tau_{\omega} \\
	|\Theta| = 1  & \\
	E[I_{\theta}] \geq 0 & \qquad \theta \in \Theta \\
	P(\phi_{x}, \alpha(\theta), t) = x & \\
	P(\phi_{x}, \phi_{y},t) = 1 - x & \\
	P(\omega(\theta), \phi_{x},t) = 1 & \qquad \theta \in \Theta \\
	P(\phi_x, \alpha(\theta),t) + P(\phi_{x}, \phi_{y},t) = 1 & \qquad \theta \in \Theta \\
\end{eqnarray}

is called \textit{conditional loop structure} and $\tau$ denotes a \textit{conditional loop} type; it is used to model a \texttt{while} and \texttt{for} programming structures inside a serverless application. 

\begin{figure}
	\centering
	\begin{tikzpicture}
		
		
		\node[circle,draw=white,minimum width = 1cm] 
		(start) at (-2,0) {};
		
		\node[circle,draw=white,minimum width = 1cm] 
		(end) at (10,0) {};
		
		\node[circle, draw, minimum width = 1.3cm]        					   
		(startLoopNode) at (0,0) {$\phi_x$};
		
		\node[circle, draw, minimum width = 1.3cm]        					   
		(endLoopNode) at (8,0) {$\phi_y$};
		
		\node[circle, draw, minimum width = 1.3cm]        					   
		(node1) at (4,2) {$\alpha(\theta)$};
		
		\node[circle, minimum width = 0.5cm, rotate=90]        					   
		(node2) at (4,3.5) {...};
		
		\node[circle, draw, minimum width = 1.3cm]        					   
		(node3) at (4,5) {$\phi_c$};
		
		% Edges...
		
		\draw[bend right,below, ultra thick,->] (startLoopNode) edge node[xshift=40]{$P(\phi_x,\alpha(\theta),t) = x$} (node1);
		
		\draw[bend right,above, ultra thick,->] (node3) edge node[xshift=-40]{$P(\phi_c,\phi_x,t) = 1$} (startLoopNode);
		
		\draw[bend right,below, ultra thick,->] (startLoopNode) edge node[]{$P(\phi_x,\phi_y,t) = 1 - x$} (endLoopNode);
		
		
		
		\draw[dashed,ultra thick, ->] (start) -- (startLoopNode);
		\draw[dashed,ultra thick, ->] (endLoopNode) -- (end);
		
		
		\draw[ultra thick, ->] (node1) -- (node2);
		\draw[ultra thick, ->] (node2) -- (node3);
		
		% Frames
		
		\node[rectangle, draw, dashed, minimum width =1.8cm, minimum height=5cm] (sub1) at (4,3.5) {};
		\node[circle,above of=sub1, yshift=50] (label1) {$\theta$};
		
		%legend
		
		\matrix [below left] at (current bounding box.north east) {
			\node [] {$x \in \mathbb{R} : 0 \leq x \leq 1$}; \\
		};
		
	\end{tikzpicture}
	\caption{Conditional loop structure in a serverless workflow.}
\end{figure}

Similarly to the branch structure, we have adopt the equation \ref{naiveProbability} to compute the transition probability $P(\phi_{x}, \alpha(\theta), t)$.

In order to compute performance data of a loop structure, $E[I_{\theta}(t)]$, that is the expected value of the number of iterations of $\mathcal{L}$ at the time $t$, is required.

To compute aforementioned information, we can model our problem using geometric distribution, which gives the probability according to which the first occurrence of success requires $k \in \N$ independent trials, each with success probability $p$ and failure probability $q = 1 - p$. In our case, if the our success corresponds to event ``\textit{we will not execute the loop body}", we know that success probability $p$ is given by:

\begin{eqnarray}
	p = P(\phi_x,\phi_y,t) = 1 - P\Big(\phi_{x}, \alpha(\theta),t\Big) \\
\end{eqnarray}

Then:

\begin{eqnarray}
	P(I_{\theta} = k) & = & p \cdot (1-p)^{k-1} \\
	& = & pq^{k-1} \\
	& = & \Big[  1 - P\Big(\phi_{x}, \alpha(\theta),t\Big) \Big] \cdot \bigg[  P\Big(\phi_{x}, \alpha(\theta),t\Big) \bigg] ^{k-1} \\
\end{eqnarray}

At any time $t$, the expected value regarding the number of iterations of $\mathcal{L}$, involving the execution of the sub-choreography $\theta$, can be computed as follows:

\begin{eqnarray}
	E[I_{\theta}(t)] & = & \sum_{k = 1}^\infty (k-1) pq^{k-1} \nonumber \\
	& = & p \sum_{k = 0}^\infty kq^{k} \nonumber \\
	& = & p \cdot \frac{q}{(1-q)^2} \nonumber \\
	& = & \dfrac{q}{p} \nonumber \\
	& = & \dfrac{P\Big(\phi_{x}, \alpha(\theta),t\Big)}{1 - P\Big(\phi_{x}, \alpha(\theta),t\Big)} 
\end{eqnarray}

At any time $t$, the response time and the cost of the conditional loop structure $\mathcal{L}$ can be computed as follows:

\begin{equation}
	RT(\mathcal{L},\textbf{x}_{\mathcal{C}}, t) \mathDef E[I_{\theta}(t)] \cdot RT(\theta_i,\textbf{x}_{\mathcal{C}}, t)
\end{equation}

\begin{equation}
	C(\mathcal{L},\textbf{x}_{\mathcal{C}}, t) \mathDef E[I_{\theta}(t)] \cdot C(\theta_i,\textbf{x}_{\mathcal{C}}, t)
\end{equation}

The exit probability of a conditional loop structure was been modeled, once again, exploiting geometric distribution with success probability equal to $P_{exit}(\theta, t)$. To be precise, exit probability is modeled as the probability that the termination of the execution of the choreography to which $\mathcal{L}$ belongs requires $\lfloor E[I_{\theta}(t)] \rfloor$  executions of $\mathcal{L}$. Formally:

\begin{eqnarray}
	P_{exit}(\mathcal{L}, t) & \mathDef & \Big(1 -P_{exit}(\theta, t)  \Big)^{\lfloor E[I_{\theta}(t)] \rfloor - 1} \cdot P_{exit}(\theta, t)\\
\end{eqnarray}

\subsubsection{Exit}

Any structure $\mathcal{E} = (\Phi',E',\Theta)$ such that:

\begin{eqnarray}
	\mathscr{T}(\alpha(\mathcal{L})) = \mathscr{T}(\phi_x) = \tau_{\alpha} \\ \mathscr{T}(\omega(\mathcal{L})) = \mathscr{T}(\phi_y) = \tau_{\omega} \\
	|\Theta| = 0  & \\
	P(\phi_{x}, \phi_{y},t) = 0 & \qquad \forall t \\
\end{eqnarray}

is called \textit{exit structure} and $\tau$ denotes a \textit{exit} type. That structure is used to model a \texttt{return} or \texttt{exit} statement inside a serverless application. 

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
		
		\node[circle,draw=white,minimum width = 1cm] 
		(start2) at (-4,0) {};
		
		\node[circle,draw,minimum width = 1cm] 
		(start) at (-2,0) {$\phi_x$};
		
		\node[circle,draw,minimum width = 1cm] 
		(end) at (2,0) {$\phi_y$};
		
		\node[circle,draw=white,minimum width = 1cm] 
		(end2) at (4,0) {};
		
		
		\draw[ultra thick,->] (start) edge node[yshift=20]{$P(\phi_x,\phi_y, t) = 0$} (end);
		
		
		% Dashed lines
		
		\draw[dashed,ultra thick, ->] (start2) -- (start);
		
		\draw[dashed,ultra thick, ->] (end) -- (end2);
		
	\end{tikzpicture}
	\caption{An exit structure in a serverless workflow.}
\end{figure}


At any time $t$, following is true that:

\begin{equation}
	RT(\mathcal{E},\textbf{x}_{\mathcal{C}}, t) = 0
\end{equation}

\begin{equation}
	C(\mathcal{E},\textbf{x}_{\mathcal{C}}, t) = 0
\end{equation}

\begin{equation}
	P_{exit}(\mathcal{E}, t) = 1
\end{equation}


\section{Serverless choreography configuration}

In order to reach his goal, our framework has to determine a so-called \textit{serverless choreography configuration}, or simply \textit{choreography configuration} or \textit{configuration}, specifying which concrete function will be invoked when the corresponding executable function is executed; moreover, its configuration's parameters, like RAM memory size or CPU power, has to be determined.

Formally, let a choreography $\mathcal{C} = (\Phi,E)$, when an invocation's request of $\mathcal{C}$ arrive on our system, the latter acts as follows:

\begin{itemize}
	\item For each $\phi \in \mathscr{F_E}(\mathcal{C})$, it selects only one concrete function $f_{\phi} \in \textbf{F}_{\phi}$, which will be effectively invoked and executed on its corresponding FaaS platform. 
	\item For each selected concrete function $f_{\phi}$, it selects a value for RAM memory size.
\end{itemize}

Clearly, as we will explain in detail later, to perform aforementioned selection, our framework has to acquire a series of function response time and charged costs when the $\mathcal{C}$ is executed using different concrete functions and memory sizes.  

\subsection{Executable function configuration}

To build a configuration for a given choreography, is clearly needed to select a configuration for its executable functions. 

Then, let $\phi \in \mathscr{F_E}(\mathcal{C})$ an executable function and $\textbf{F}_{\phi}$ the corresponding implementation-set, formally an \textit{executable function configuration} $x_{\phi}$ for the executable function $\phi$ is a two-dimensional vector defined as follows:

\begin{equation}
	x_{\phi} = (f_{\phi},m) \in f_{\phi} \times \textbf{M}_{f_{\phi}} \subseteq \textbf{F}_{\phi} \times \N
\end{equation}

where:

\begin{itemize}
	\item $f_{\phi} \in \textbf{F}_{\phi}$  denotes a particular concrete function implementing the executable function $\phi$.
	\item $m \in \textbf{M}_{f_{\phi}}$ represents the allocated memory size during the execution of $f_{\phi}$, where $\textbf{M}_{f_{\phi}} \subseteq \N$ is the set holding all available memory size configurations allowed by provider where the concrete function $f_{\phi}$ is executed.
\end{itemize}

\subsection{Serverless choreography configuration}

Let $n,k \in \N \setminus \left\lbrace 0 \right\rbrace$ and a choreography $\mathcal{C} = (\Phi,E)$ such that $|\Phi| = n$ and $|\mathscr{F_E}| = k$ where $k \leq n$.

Formally, a \textit{serverless choreography configuration} $\textbf{X}_{\mathcal{C}}$ for the choreography $\mathcal{C}$ is a vector such that:

\begin{eqnarray}
	\textbf{x}_{\mathcal{C}} & \mathDef & \left\lbrace x_{\phi_{1}}, \ldots, x_{\phi_{k}} \right\rbrace \nonumber \\ 
	& \in & \left\{  \left\{ \bigcup_{j=1}^{|\textbf{F}_{\phi_{1}}|} f_{\phi_{1_j}} \times \textbf{M}_{f_{\phi_{1_j}}} \right\} \times \ldots \times \left\{ \bigcup_{j=1}^{|\textbf{F}_{\phi_{k}}|} f_{\phi_{k_j}} \times \textbf{M}_{f_{\phi_{k_j}}} \right\} \right\}  \nonumber \\
	& = & \Cross_{i = 1}^k \left\{ \bigcup_{j=1}^{|\textbf{F}_{\phi_{i}}|} f_{\phi_{i_j}} \times \textbf{M}_{f_{\phi_{i_j}}} \right\} \nonumber \\
	& \subseteq & \Cross_{i = 1}^k \left\{ \textbf{F}_{\phi_{i}} \times \mathbb{N} \right\} = \textbf{X}_{\mathcal{C}}
\end{eqnarray}

where:

\begin{itemize}
	\item $x_{\phi_{i}}$ represents the executable function configuration for the executable function $\phi_{i}$, for some  $i \in \N \cap \left[ 1, k \right]$.
	
	\item $f_{\phi_{i_j}}$ represent the $j$-th concrete function implementing the executable function $\phi_{i}$, for some  $i \in \N \cap \left[ 1, k \right]$ and $j \in \N \cap \left[ 1, |\textbf{F}_{\phi_{i}}| \right]$.
	
	\item $\textbf{M}_{f_{\phi_{i_j}}} \subseteq \N$ denotes the set containing all available memory size options, allowed by provider, during the execution of the concrete function $f_{\phi_{i_j}}$, for some $i \in \N \cap \left[ 1, k \right]$ and $j \in \N \cap \left[ 1, |\textbf{F}_{\phi_{i}}| \right]$
	
	\item $\textbf{X}_{\mathcal{C}}$ represents the so-called \textit{solutions space}, which includes all possible choreography configurations.
	
	
\end{itemize}

\section{Concrete function's performance evaluation}

Clearly, in order to select an appropriate serverless choreography configuration able to effectively guarantee SLA requirements specified by users, we need firstly to evaluate the performance of concrete serverless, in terms of average values regarding response time and charged costs.

However, to develop both an analytical way and a software framework capable to evaluate concrete serverless function performance, we first need to understand
how they are managed by FaaS platforms.

\subsection{Function instances}

According to serverless computing paradigm, computation is done inside isolated environments, provided by virtualization solutions such as virtual machines, containers, unikernels or even processes, called \textit{function instances}.

As known, these instances can be considered as lightweight servers which management, provisioning and the fulfillment of any other infrastructure issues are the responsibilities of serverless computing platform provider.

We can identify three states for each function instance:

\begin{description}
	
	\item[Initialization State] which happens when the infrastructure is spinning up new function instance, which is needed to handle incoming requests. 
	
	A function instance will remain in the initializing state until it is able to handle incoming requests. According to FaaS policies, the time spent in this state is not billed. 
	
	\item[Idle State] After the fulfillment of all initialization tasks or when the processing of a previously received serverless function invocation request is over, the serverless platform keeps a function instances in idle state.
	
	In that way, the FaaS provider keep aforementioned function instance able to handle future invocation request faster, since no initialization task is needed to be performed.
	
	However, FaaS platform provider keeps a function instance in idle state for a limited amount of time; after that, all resources used to execute the function instance will be deallocated.
	
	The user is not charged for an instance that is in the idle state.
	
	\item[Running State] When an invocation request is submitted to an function instance, the latter goes into the running state, according to which aforementioned request is parsed and processed.
	
	Clearly, the time spent in the running state is billed by the provider.
	
\end{description}

In order to be clear, we will use the expression \textit{warm pool} when referring to the set of all function instances whose state is either idle or running state.

\subsection{FaaS auto-scaling technique}

According to serverless computing paradigm, the FaaS platform provider has the responsibility to manage the amount of function instances required to allow users to perform their computation.

In this dissertation, we assume that all FaaS providers adopt an auto-scaling technique called \textit{scale-per-request}.

If such technique is adopted, when a request comes in, only one of the following events can occur:

\begin{description}
	
	\item[Warm start] if there is at least one function instance in idle state, the FaaS platform reuses it to serve the incoming request without launching a new one.
	
	\item[Cold start] If the warm pool is empty or busy, that is there are no serverless function instances in idle state able to serve an newly incoming request, FaaS platform will trigger the launching of a new function instance, which will be added to the warm pool.
	
	As said previously, from the FaaS provider point of view, this operation requires the start of a virtualized and isolated execution environment (i.e. virtual machine, container and so on) where user code will be run; in any case, regardless of the virtualization solution adopted, a cold start introduces a very important overhead to the response time experienced by users. 
	
	Therefore, due to initialization tasks performed to spin up a new function instance, cold starts could be orders of magnitude longer than warm starts for some applications; therefore, too many cold starts could impact the application’s responsiveness and user experience.
\end{description} 

As long as, for a given instance, requests are received within an interval time less than an \textit{expiration threshold}, the function instance will be not deallocated.

At the same time, for each instance, at any moment in time, if a request has not been received in the last \textit{expiration threshold} units of time, it will be expired and thus terminated by the platform, and the consumed resources will be released. 

This technique is currently adopted by the vast majority of well-known public serverless computing platforms, like AWS Lambda, Google Cloud Functions, IBM Cloud Functions, Apache OpenWhisk and Azure Functions. 

\subsection{FaaS request routing}

In this dissertation, we assume that, in order to minimize the number of
function instances that are kept warm and thus to free up system resources, the FaaS routes requests giving priority to instances which they have been idle for less time.

In other words the FaaS request routeing give low priority to function instances being in idle state for long time, increasing thus the chances of resource deallocation referring aforementioned instances.

\subsection{Concurrency limit}

Any FaaS platform imposes a limitation on the number of serverless function instance runnable at the same time; this limit is generally known as \textit{concurrency level} or \textit{concurrency limits}. 

Clearly, this kind of limitation is needed to assure enough resources for all users using the services provided by FaaS platform. However, despite all FaaS providers impose aforementioned limitation, these restriction are applied differently.

Informally speaking, there are two type of concurrency limit models:

\begin{description}
	\item[Global (Per-Account) Concurrency Model] according to which invocation threshold is shared by all serverless functions belonging to a given resource owner. 
	
	For example, in $2022$, this approach is adopted both by AWS Lambda and IBM Cloud Functions, which do not allow more than $1000$ serverless concrete function in running state at the same time.
	
	\item[Local (Per-Function) Concurrency Model] where, opposed to global concurrency model, any invocation threshold is applied only on individual concrete functions. 
	
	This approach is adopted by Google Cloud Functions \footnote{Despite there is no explicitly mentioned global concurrency limit, previous studies have observed a kind of global concurrency limit varying between $1000$ and $2000$.}
	
\end{description}

\subsection{FaaS platform modeling}

According to our model point of view, at any time $t$, any FaaS platform provider acts as a ``\textit{set}'' of $M/G/K(t)_{\textbf{C}_{max}}/K(t)_{\textbf{C}_{max}}$ queueing systems, also called \textit{$K(t)$-server loss systems}, where:

\begin{itemize}
	
	\item $\textbf{C}_{max} \in \N \setminus \left\{0\right\}$ is a scalar representing the queuing system's concurrency limit.
	
	In other words, $\textbf{C}_{max}$ is the maximum size of the warm pool, therefore it represents the maximum number of function instances being in running state simultaneously at time $t$. 
	
	Please note that $\textbf{C}_{max}$ not necessary coincide with a global concurrency limit because it can represents a local concurrency limit too. We will give more details about this very soon.
	
	\item At any time $t \geq 0$, there are $K(t)$ function instances. Since each instance can process only one request, we can say that the system has capacity for $K(t)$ invocation requests in total. The number of function instance, and consequently system's capacity, can change over time. 
	
	At any time $t$, following condition must be hold:
	
	\begin{equation}
		0 \leq K(t) \leq {C}_{\textbf{max}}
	\end{equation}
	
	\item Since no queue is involved, if the maximum concurrency level is reached, that is $K(t) = {C}_{\textbf{max}}$, any incoming request at time $t$, that sees all $K(t)$ function instances in running state, will be permanently dropped \footnote{Indeed, a queue exists; there are previous studies stating that all FaaS platform providers adopt a scheduling policies based on \textit{first-come-first-served} (FIFO) algorithms. Anyways, since the built of a QoS-aware scheduler is out of the scope of this dissertation, in order to meet QoS objectives, we have decided to drop any request exceeding maximum concurrency level instead of relying on scheduling policies provided by FaaS platforms.}.
	
	\item No priority is considered among incoming request.
	
	\item Service times have a general distribution while a Poisson arrival process is assumed.
\end{itemize} 

\begin{figure}[h!]
	\centering
	\begin{tikzpicture}
		
		
		\node[circle, minimum width = 1cm]        					   
		(startNode) at (-8,0) {$\lambda$};
		
		\node[circle, inner sep=0pt]        					   
		(startNode2) at (-4,0) {};
		
		% Node
		
		\node[circle, draw, minimum width = 1.5cm]        					   
		(f1) at (3,3) {$f_1$};
		\node[circle, draw, minimum width = 1.5cm]       					   
		(f2) at (3,1) {$f_2$};
		\node[circle, minimum width = 1.5cm]       					   
		(fxxx) at (3,-1) {$\cdots$};
		\node[circle, draw, minimum width = 1.5cm]       					   
		(fn) at (3,-3) {$f_n$};
		\node[circle, draw, minimum width = 1.5cm]       					   
		(fn2) at (3,-5) {$f_{n+1}$};
		
		\node[circle, dashed, blue, draw, minimum width = 1.5cm]       					   
		(fn2fake) at (-4,-5) {$f_{n+1}$};
		
		\node[circle, dashed, minimum width = 1.5cm]       					   
		(fn2fakeExit) at (-4,-8) {};
		
		% Exit node
		
		\node[circle, minimum width = 1.5cm]        					   
		(Ef1) at (5.5,3) {};
		\node[circle, minimum width = 1.5cm]       					   
		(Ef2) at (5.5,1) {};
	

		\node[circle, minimum width = 1.5cm]       					   
		(Efn) at (5.5,-3) {};
		\node[circle, minimum width = 1.5cm]       					   
		(Efn2) at (5.5,-5) {};
		
	
		% EDGE
	
		\draw[dashed, ultra thick,->] (startNode) -- (startNode2);
		\draw[dashed, ultra thick,->] (startNode2) -- (f1);
		\draw[dashed, ultra thick,->] (startNode2) -- (f2);
		\draw[dashed, ultra thick,->] (startNode2) -- (fn);
		\draw[dashed, ultra thick,->,blue] (fn2fake) -- (fn2);
		\draw[dashed, ultra thick,->, blue] (startNode2) -- (fn2fake);
		
		% Exit EDGE
		
		\draw[dashed, ultra thick,->] (f1) -- (Ef1);
		\draw[dashed, ultra thick,->] (f2) -- (Ef2);
		\draw[dashed, ultra thick,->] (fn) -- (Efn);
		\draw[dashed, ultra thick,->] (fn2) -- (Efn2);
		\draw[dashed, ultra thick,->,red] (fn2fake) -- (fn2fakeExit);
		
		%\node[circle,below of=sub1, yshift=10] (label1) {$\theta_1$};
		\node[rectangle, draw, dashed, minimum width =2cm, minimum height=2cm] (sub1) at (3,-5) {};
		
		\node[rectangle, draw, dashed, minimum width =2cm, minimum height=8cm] (sub1) at (3,0) {};
		
		%\draw[ultra thick,->] (ParNode1) edge node[xshift=15 ,yshift=15]{$P(\phi_2,\phi_4) = 1$} (ParEndNode);
		%\draw[ultra thick,->] (ParNode2) edge node[xshift=15 ,yshift=-15]{$P(\phi_3,\phi_4) = 1$} (ParEndNode);
		
		%\draw[dashed,ultra thick, ->] (start) -- (ParStartNode);
		%\draw[dashed,ultra thick, ->] (ParEndNode) -- (end);
		
		
		\node[text width=6.5cm] at (2.5,4.5) {\textbf{Warm Pool having capacity} $n$};
		
		\node[text width=5cm, blue] at (-0.5,-6.5) {\textbf{ Instance added to warm pool increasing its capacity by one and causing a cold start.}};
		
		\node[text width=5.5cm, blue] at (-7,-3) {\textbf{Blocking due to lack of idle function instances}};
		
		\node[text width=6.3cm, red] at (-7.5,-6.5) {\textbf{Request dropped if maximum concurrency limit is reached}};
		
		
		%legend
		
		%\matrix [below left] at (current bounding box.north east) {
		%	\node [] {$\phi_1 = p_{\alpha} \in \mathscr{F_O}$}; \\
		%	\node [] {$\phi_3 = p_{\omega} \in \mathscr{F_O}$}; \\
		%	\node [] {$\phi_2, \phi_3 \in \mathscr{F_E}$}; \\
		%};
		
		
	\end{tikzpicture}
	\caption{Parallel structure in the serverless workflow.}
\end{figure}

\subsection{Concrete function's swarm}

To complete the formalization of our model and understand how concrete function performances are evaluated, is necessary to introduce another very important concept needed to make our model compatible with all concurrency limit models adoptable by FaaS providers.

Let $l \in \N$, $R$ a resource owner, $P$ a serverless computing platform provider and $\Omega_{P_R}$ the set of all concrete serverless functions hosted on $P$ belonging to $R$.

A \textit{concrete function's swarm}, or simply \textit{swarm}, is the set $\omega_{P_R}^{(l)} \subseteq \Omega_{P_R}$ containing all concrete functions sharing the same limit $l$ in term of the max number of serverless function instance runnable at the same time by $P$; in that case, we call $l$ as \textit{swarm's concurrency limit}.

In simple terms, only at most $l$ executions of any concrete functions belonging to $\omega_{P_R}^{(l)}$ can be performed simultaneously by $P$. The value of $l$ depends on the concurrency model adopted by $P$. 

\begin{itemize}
	
	\item If $P$ imposed a \textit{per-function} limit, then $|\omega_{P_{R_l}}| = 1$, that is, $\omega_{P_R}^{(l)}$ contains only one function and $l$ will represent the provider's per-function limit. 
	
	In that case, to each concrete function belonging to $\omega_{P_R}^{(l)}$ corresponds a dedicated $M/G/K(t)_{l}/K(t)_{l}$ queuing system, which will serve all invocation request regarding only its corresponding concrete function.  
	
	\item If $P$ imposed a \textit{per-account} limit, then $\omega_{P_R}^{(l)} = \Omega_{P_R}$, that is the swarm includes all concrete serverless concrete function deployed on $P$ by $R$, while $l$ represents the provider's global concurrency limit. 
	
	In that case, there is only one $M/G/K(t)_{l}/K(t)_{l}$ queuing system serving all invocation requests regarding any concrete function belonging to $\Omega_{P_R}$. 

\end{itemize}

From now, the sets $\omega_{P_R}^{(l)}$ and $\Omega_{P_R}$ will be briefly and respectively denoted by $\omega_{P}^{(l)}$ and $\Omega_{P}$ when no confusion can arise about the resource owner $R$. Moreover, we will adopt $\textbf{Q}_{\omega_{P}^{(l)}}$ notation to refer to the $M/G/K(t)_{l}/K(t)_{l}$ queuing system serving all invocation requests of any concrete function belonging to $\omega_{P}$. 

\subsubsection{Cold start probability}

According to our model, if maximum concurrency level is not exceed, the rejection of a request by the warm pool will trigger a cold start, adding a new function instance to the warm pool in order to handle aforementioned request. 

Now we have to know the probability according to which a request is rejected by the warm pool; in other words, we must to compute the \textit{cold start probability}. 

Formally, let $l \in \N$, $P$ a serverless computing platform provider, $\omega_{P}^{(l)}$ a swarm and $\textbf{Q}_{\omega_{P}^{(l)}}$ its corresponding queuing system on the FaaS platform provider. Following functions will be used:

\begin{itemize}
	\item $R(\textbf{Q}_{\omega_{P}^{(l)}}, t)$ representing the function returning the number of running state function instances at time $t$ on $\textbf{Q}_{\omega_{P}^{(l)}}$.
	
	\item $K(\textbf{Q}_{\omega_{P}^{(l)}}, t)$ returning instead the number of function instances at time $t$ deployed on $\textbf{Q}_{\omega_{P}^{(l)}}$.
\end{itemize}

In this section, to simplify our notations, since no confusion can arise about the queuing system, $R(\textbf{Q}_{\omega_{P}^{(l)}}, t)$ will be briefly denoted as $R(t)$ and $K(\textbf{Q}_{\omega_{P}^{(l)}}, t)$ as $K(t)$.

When a new invocation's request of any function belonging to $\omega_{P}^{(l)}$ arrives on $\textbf{Q}_{\omega_{P}^{(l)}}$, one of the following events can occur:

\begin{itemize}
	\item If $K(t_n) < l$ and $R(t_n) < K(t_n)$, that is the number of function instances into the warm pool is less than the swarm's maximum concurrency level and there are some function instance in idle state, a warm start will occur, resulting that $R(t_{n+1}) = R(t_n) + 1$.
	
	
	\item If $K(t_n) < l$ and $R(t_n) = K(t_n)$, that is the number of function instances into the warm pool is once again less than the swarm's maximum concurrency level but there is no function instance in idle state, a cold start will occur and the size of the warm pool will be increased by one, that is $K(t_{n+1}) = K(t_n) + 1$.
	
	The newly accepted request will be managed by the newly spinned up function instance, causing that $R(t_{n+1}) = R(t_n) + 1$.
	
	\item If $K(t) = R(t) = l$, that is the warm pool reaches is maximum possible size according to FaaS policies, aforementioned request will be rejected.
\end{itemize}

If at time $t_n$ a concrete function execution's has been completed, that event will cause $R(t_{n+1}) = R(t_n) - 1$. 

Conversely, if a function instance has not been received any request during the last expiration threshold units of time, it will be expired causing the decreasing of warm pool size, that is $K(t_{n+1}) = K(t_n) - 1$.

Finally, at any time $t$, the \textit{cold start probability} $\textbf{P}_{\omega_{P_R}^{(l)}}(t)$ referring to the invocations of any function belonging to the swarm $\omega_{P_R}^{(l)}$, can be formally defined as the probability that an arrival request of invocation finds all function instances of the warm pool busy; using Erlang-B formula, that probability can be calculated as follows:

\begin{equation}
	\displaystyle \textbf{P}_{\omega_{P_R}^{(l)}}(t) = \dfrac{\rho(t)^{K(t)}}{K(t)!} \cdot \Bigg( \displaystyle\sum_{j=0}^{K(t)} \dfrac{\rho(t)^j}{j!}\Bigg)^{-1}
\end{equation}

where:

\begin{itemize}
	
	\item $\displaystyle \rho(t) = \frac{\lambda(t)}{\mu_{w}}$ represents system utilization or load of the warm pool at time $t$ where:
	
	\begin{itemize}
		
		\item $\lambda(t)$ represents the \textit{average arrival rate} at time $t$, that is the rate at which invocation requests, regarding serverless concrete functions belonging to $\omega_{P_R}^{(l)}$, arrive to our system. It is expressed in $invocations \cdot s^{-1}$.
				
		Since arrival rate varies during the day, $\lambda(t)$ must be estimate at runtime, over multiple time intervals spent observing our system.
		
		We have adopt the exponential moving average based approach to compute an estimation of $\lambda(t)$. Periodically, at any time $t$, our framework compute the number $Y_t$ of all received invocation requests for any function belonging to $\omega_{P_R}^{(l)}$ within last second. Then $\lambda(t)$ is estimated as follows:
		
		\begin{equation}
			\lambda(t) \mathDef
			\begin{cases}
				Y_o & \text{if}\ t = 0\\ 
				\alpha \cdot Y_t + (1 - \alpha) \cdot \lambda(t-1) & \text{if}\ t > 0\\
				
			\end{cases}\,.
		\end{equation}
		
		\item $\mu_{w} = E[S_w]$ represents the \textit{warm start average service rate}, that is the rate at which executions requests are served when a warm start is occurred, while $E[S_w]$ is the \textit{warm start average time}, which is the average time required to complete aforementioned request.
		
	\end{itemize}
	
\end{itemize}

\subsubsection{Choreography's swarm}

Let $\mathcal{C} = (\Phi,E)$ a choreography, to build its configuration $\textbf{x}_{\mathcal{C}}$, we firstly need to identify all swarms corresponding to each concrete function $f_{\phi} \in \textbf{F}_{\phi}$, for all $\phi \in \mathscr{F_E}$, because we have to check if we can execute all concrete functions specified in $\textbf{x}_{\mathcal{C}}$, without exceeding maximum concurrency level on each FaaS platforms where aforementioned functions will be executed.

Informally, a \textit{choreography's swarm} $\widetilde{\textbf{S}}_{\mathcal{C}}$ represents the set of all swarms corresponding to all concrete functions used by $\mathcal{C}$. 

Formally, let $m \in \N$ the total number of FaaS platform providers where at least one concrete function is deployed by a given resource owner, a choreography's swarm can be defined as follows:

\begin{equation}
	\widetilde{\textbf{S}}_{\mathcal{C}} \mathDef \left\{ \omega_{P}^{(l)} \in \bigcup_{i=1}^n \Omega_{P_i} : \exists f_{\phi} \in \omega_{P}^{(l)} \text{ such that } f_{\phi} \in \textbf{F}_{\phi}, \forall \phi \in \mathscr{F_E}(\mathcal{C}) \right\}
\end{equation}

Please note that, let $\mathcal{C}_1 = (\Phi_1,E_1)$ and $\mathcal{C}_2 = (\Phi_2,E_2)$ two different choreographies belonging to a same resource owner, is generally verified that:

\begin{equation}
	\widetilde{\textbf{S}}_{\mathcal{C}_1} \cap \widetilde{\textbf{S}}_{\mathcal{C}_2} \neq \emptyset
\end{equation}

that is a swarm can be shared by multiple choreographies.

\subsection{Concrete function performance computation}

Finally, we can introduce how concrete function performance can be computed according to our model.

Let $x_{\phi} = (f_{\phi},m)$ an executable function configuration such that $f_{\phi} \in \omega_{P_R}^{(l)} \subset \textbf{F}_{\phi}$ represents a concrete function implementing the executable function $\phi \in \mathscr{F_E}(\mathcal{C})$ and belonging to the swarm $\omega_{P_R}^{(l)}$, where $l \in \N$ is its concurrency limit. Moreover, $m \in \textbf{M}_{f_{\phi}} \subseteq \N$ denotes the value of the memory size selected by our framework for the execution of $f_{\phi}$.

Let's define following functions:

\begin{itemize}	
	\item $C : \textbf{F}_{\phi} \times \textbf{M}_{f_{\phi}} \times \mathbb{R}^+ \to \mathbb{R}^+$ is the \textit{cost function} for any serverless concrete functions belonging to the implementation-set $\textbf{F}_{\phi}$. 
	
	It returns the \textit{average cost} paid by users when $f_{\phi}$ is executed using an allocated memory size equal to $m$ at time $t$. 
		
	It can be defined as follows:
	
	\begin{eqnarray}
		C(x_{\phi},t) & \mathDef & C(f_{\phi},m,t) \nonumber \\
		& = & C_{avg}^{(c)}(f_{\phi}, m,t) \cdot \textbf{P}_{\omega_{P_R}^{(l)}}(t) + \\
		& & \qquad C_{avg}^{(w)}(f_{\phi}, m,t) \cdot \Big( 1 - \textbf{P}_{\omega_{P_R}^{(l)}}(t)  \Big) \nonumber
	\end{eqnarray}
	
	where:
	
	\begin{itemize}
		\item At any time $t$. when $f_{\phi}$ is executed with an allocated memory size equal to $m$, 
		$C_{avg}^{(c)}(f_{\phi}, m,t)$ represents the average cost paid when a cold start is occurred while, conversely, $C_{avg}^{(w)}(f_{\phi}, m,t)$ represents the average cost paid in case of warm start.
	\end{itemize}
	
	
	\item $RT : \textbf{F}_{\phi} \times \textbf{M}_{f_{\phi}} \times \mathbb{R}^+ \to \mathbb{R}^+$ is the \textit{delay function} for any serverless concrete functions belonging to the implementation-set $\textbf{F}_{\phi}$. 
	
	It returns the \textit{average response time} paid by users when $f_{\phi}$ is executed using an allocated memory size equal to $m$ at time $t$. 
	
	It can be defined as follows:
	
	\begin{eqnarray}
		RT(x_{\phi},t) & \mathDef & RT(f_{\phi},m,t) \nonumber \\
		& = & RT_{avg}^{(c)}(f_{\phi}, m,t) \cdot \textbf{P}_{\omega_{P_R}^{(l)}}(t) + \\ 
		& & \qquad RT_{avg}^{(w)}(f_{\phi}, m,t) \cdot \Big( 1 - \textbf{P}_{\omega_{P_R}^{(l)}}(t)  \Big) \nonumber
	\end{eqnarray}

	where:
	
	\begin{itemize}
		\item $RT_{avg}^{(c)}(f_{\phi}, m,t)$ represents the average response time occurred when a cold start is occurred and $RT_{avg}^{(w)}(f_{\phi}, m,t)$ is the one occurred in case of in case of warm start.
	\end{itemize}
\end{itemize}

Obliviously, our framework's duty is to determine $RT_{avg}^{(c)}(f_{\phi}, m,t)$, $RT_{avg}^{(w)}(f_{\phi}, m,t)$, $C_{avg}^{(c)}(f_{\phi}, m,t)$ and $C_{avg}^{(w)}(f_{\phi}, m,t)$ using time series data, containing historical performance data of $f_{\phi}$ under different memory configurations, collected by our logging framework. To compute aforementioned estimations, we have adopt the exponential moving average based approach.

For instance, referring to the computation of $RT_{avg}^{(c)}(f_{\phi}, m,t)$, let $Y_t$ the response time of $f_{\phi}$ when it is executed with memory size $m$ at a given time $t$, following formula was been used:

\begin{equation}
	RT_{avg}^{(c)}(f_{\phi}, m,t) \mathDef
	\begin{cases}
		Y_o & \text{if}\ t = 0\\ 
		\alpha \cdot Y_t + (1 - \alpha) \cdot \lambda(t-1) & \text{if}\ t > 0\\
		
	\end{cases}\,.
\end{equation}

\section{Executable function's performance evaluation}

In this section, we will describe how to evaluate the performance of any executable serverless, in terms of average values regarding response time and charged costs, when it is executed. 

Supposing to have a choreography $\mathcal{C} = (\Phi,E)$, let $x_{\phi_{i_{j}}} = (f_j, m_j)$ an executable function configuration for $\phi_i \in \mathscr{F_E}(\mathcal{C})$, for some $i \in \N \cap [1,|\mathscr{F_E}(\mathcal{C})|]$ and $j \in \N \cap [1,|\textbf{F}_{\phi_{i}} \times \N|]$. In order to evaluate the performance of $\phi$, we have to introduce some useful notations and functions as follows:

\begin{itemize}
	\item For some $n \in \N$, $\widehat{\theta_k}^{(\phi_i)} \mathDef \left\{ \theta^{(\phi_i)}_1, \ldots, \theta^{(\phi_i)}_n \right\}$ represents the set of all sub-choreographies of $\mathcal{C}$ such that:
	
	\begin{equation}
		\phi_i \in \mathscr{F_E}(\theta^{(\phi_i)}_1)
	\end{equation}
	
	\begin{equation}
		\theta^{(\phi_i)}_n = \mathcal{C}
	\end{equation}
	
	\begin{equation}
		\theta^{(\phi_i)}_k = (\Phi_k, E_k) \text{ is a sub-choreography of } \theta^{(\phi_i)}_{k+1} \qquad \forall k \in \left[1;n-1\right]
	\end{equation}

	\item $E[I_{\phi_i}(t)]$ represents the expected value regarding the number's invocation of the executable function $\phi_i$ at time $t$, which can be defined as follows.
	
	\begin{equation}	
		E[I_{\phi_i}(t)] \mathDef \left\{
		\begin{array}{lcr}
			1 & \text{\textit{if}} & n = 1 \\ 
			\displaystyle \prod_{k = 2}^n E[I_{\theta_{k-1}^{(\phi_i)}}(t)] & \text{\textit{if}} & n \geq 2 \\
		\end{array} \right.
	\end{equation}
	
	\item $P_{exe}^C(\theta^{(\phi_i)}_k, t)$ is the probability to execute the entry point $\alpha(\theta^{(\phi_i)}_k)$ of the choreography $\theta^{(\phi_i)}_k$ when the function $pred(\alpha(\theta^{(\phi_i)}_k))$ is executed at time $t$. Obliviously, it is equal to the transition probability associated to the edge starting from $pred( \alpha(\theta^{(\phi_i)}_{k}) )$ and ending to $\alpha(\theta^{(\phi_i)}_{k}))$. Formally:
	
	\begin{equation}	
		P_{exe}^C(\theta^{(\phi_i)}_k, t) \mathDef \left\{
		\begin{array}{lcr}
			1 & \text{\textit{if}} & n = 1 \\ 
			P \bigg(  pred\Big( \alpha(\theta^{(\phi_i)}_{k}) \Big), \alpha(\theta^{(\phi_i)}_{k}) \bigg) & \text{\textit{if}} & n \geq 2 \\
		\end{array} \right.
	\end{equation}
	
	
	\item $P_{exe}^F(\theta^{(\phi_i)}_k, \phi, t)$ is the probability to execute $\phi \in \mathscr{F_E}(\theta_k)$ when $\alpha(\theta_k)$ is executed at time $t$. 
	
	In order to compute this probability, is necessary to process any orchestration function $\phi_x \in \Pi(\alpha(\theta_k),\phi)$, in order to obtain a sort of pipeline choreography from $\alpha(\theta_k)$ to $\phi$.
	
	Then, after performing the workflow simplification process, we will obtain a set $\Phi_k' = \left\{\phi_1, \ldots, \phi_n \right\} \subseteq \Phi_k$ such that:
	
	\begin{eqnarray}
		succ(\phi_x) & = &  \phi_{x+1}  \qquad x \in \SetFromOneTo{n -1} \\ \phi_n & = & \phi
	\end{eqnarray}
	
	Finally:
		
	\begin{equation}
		P_{exe}^F(\theta^{(\phi_i)}_k, \phi, t) \mathDef \prod_{i = 1}^{n-1} (1 - P_{exit}(\phi_i,t))
	\end{equation}
	


\item We will use $\Gamma_{\theta_k^{(\phi_{i})}}(t)$ to denote the probability to execute the entry point of the sub-choreography $\theta_k^{(\phi_{i})}$ at time $t$.

\begin{equation}
	\Gamma_{\theta_k^{(\phi_{i})}}(t) \mathDef P_{exe}^C(\theta^{(\phi_i)}_{k},t) \cdot \Bigg( \prod_{y = k + 1}^{n}  P_{exe}^C(\theta^{(\phi_i)}_{y},t) \cdot P_{exe}^{F}(\theta^{(\phi_i)}_{y}, pred(\alpha(\theta^{(\phi_i)}_{y-1})),t) \Bigg)
\end{equation}

\end{itemize}

Finally, we can compute response time $rt_{\phi_{i_{j}}}(t)$ and charged cost $c_{\phi_{i_{j}}}(t)$, regarding the executable function $\phi_i \in \mathscr{F_E}(\mathcal{C})$ with configuration $x_{\phi_{i_{j}}}$, using following formulas:

\begin{eqnarray}
	c_{\phi_{i_{j}}}(t) & \mathDef & \left\{ 
	\begin{array}{lcl}
		C(x_{\phi_{i_{j}}},t) \cdot P_{exe}^{F}(\theta^{(\phi_i)}_1, \phi_i,t) & \text{\textbf{if}} & m = 1 \\ 
		C(x_{\phi_{i_{j}}},t) \cdot P_{exe}^{F}(\theta^{(\phi_i)}_1, \phi_i,t) \cdot E[I_{\phi_i}(t)] \cdot \Gamma_{\theta_1^{(\phi_{i})}}(t) & \text{\textbf{if}} & m \geq 2 \\ 
	\end{array} \right.
\end{eqnarray}

\begin{eqnarray}
	rt_{\phi_{i_{j}}}(t) & \mathDef & \left\{ 
	\begin{array}{lcl}
	RT(x_{\phi_{i_{j}}},t) \cdot P_{exe}^{F}(\theta^{(\phi_i)}_1, \phi_i,t) & \text{\textbf{if}} & m = 1 \\ 
	RT(x_{\phi_{i_{j}}},t) \cdot P_{exe}^{F}(\theta^{(\phi_i)}_1, \phi_i,t) \cdot E[I_{\phi_i}(t)] \cdot \Gamma_{\theta_1^{(\phi_{i})}}(t) & \text{\textbf{if}} & m \geq 2 \\ 
\end{array} \right.
\end{eqnarray}

Let $\left\langle (RT,w_{RT}),(C,w_{C}) \right\rangle$ SLAs constraints specified by a customer, for any executable function configuration $x_{\phi_{i_j}}$, its \textit{score} $p_{\phi_{i_{j}}}(t)$ is determined as follows;

\begin{equation}
	p_{\phi_{i_{j}}}(t) \mathDef w_{RT} \cdot \dfrac{t_{\phi_{i_{\textbf{MAX}}}}(t) - rt_{\phi_{i_{j}}}(t)}{t_{\phi_{i_{\textbf{MAX}}}}(t) - rt_{\phi_{i_{\textbf{MIN}}}}(t)} + w_{C} \cdot \dfrac{c_{\phi_{i_{\textbf{MAX}}}}(t) - c_{\phi_{i_{j}}}(t)}{c_{\phi_{i_{\textbf{MAX}}}}(t) - c_{\phi_{i_{\textbf{MIN}}}}(t)}
\end{equation}

where:

\begin{itemize}
	\item $rt_{\phi_{i_{\textbf{MIN}}}}(t)$ and $rt_{\phi_{i_{\textbf{MAX}}}}(t)$ represent, respectively, the minimum and maximum response time values regarding the execution of all concrete function implementing $\phi_i$. 
	
	In other words, they represent the minimum and maximum resource requirement, referring to the response time, of all items belonging to the $i$-th group of our problem.
	
	\item $c_{\phi_{i_{\textbf{MIN}}}}(t)$ and $c_{\phi_{i_{\textbf{MAX}}}}(t)$ represent, respectively, the minimum and maximum cost values spent by all concrete function implementing $\phi_i$.
\end{itemize}




\section{Choreography's performance evaluation}

In this section, we will briefly explore the analytical methodology to compute the average end-to-end response time and charged cost of generic serverless applications, having branches, loops or even workflow's portions executed in parallel, when a choreography configuration is given.

\subsection{Pipeline choreography performance}

Let $\mathcal{C} = (\Phi,E)$ a pipeline choreography as defined in section \ref{PipelineDefinitionSection} and $\textbf{x}_{\mathcal{C}} \in \textbf{X}_{\mathcal{C}}$ a choreography configuration. Fortunately, exploiting pipeline choreography's properties, is very simple to obtain a performance evaluation when $\textbf{x}_{\mathcal{C}}$ is given. 

Let $\Phi = \left\{\phi_1, \ldots, \phi_n \right\}$ such that:

\begin{equation}
	succ(\phi_x) = \phi_{x+1}  \qquad x \in \SetFromOneTo{n -1}
\end{equation}


Then, at any time $t$, following equations can be used:

\begin{eqnarray}
	RT(\mathcal{C},\textbf{x}_{\mathcal{C}}, t) & \mathDef &  RT(x_{\phi_1},t) + \sum_{x = 2}^n P_{exe}^F(\mathcal{C},\phi_{x-1},t) \cdot RT(x_{\phi_x},t) \label{eq:perfF1} \\
	C(\mathcal{C},\textbf{x}_{\mathcal{C}}, t) & \mathDef &  C(x_{\phi_1},t) + \sum_{x = 2}^n P_{exe}^F(\mathcal{C},\phi_{x-1},t) \cdot C(x_{\phi_x},t) \label{eq:perfF2}
\end{eqnarray}

\subsection{Serverless choreography's structures}

Unfortunately, equations \ref{eq:perfF1} and \ref{eq:perfF2} can be used only dealing with pipeline type choreographies, therefore they are not suitable for general choreographies having branch or loops. 

A very simple approach to obtain a performance evaluation of any choreography's kind, is to simplify the serverless workflow in a such way to obtain a pipeline type choreography, allow us to apply aforementioned equations. To do that, exploiting different methods for different \ItalicQuotMark{structures}, the performance model trims the graph associated with a choreography by removing or modifying vertices and edges in order to convert it into a pipeline choreography. Firstly, we must explain what we mean for structure.



\subsection{Generic choreography performance}

\begin{algorithm}
	
	$\mathcal{C}^* \leftarrow \mathcal{C}$\;
	\While{$\mathcal{C}^*$ is not a pipeline choreography}{
		
		\BlankLine
		
		\texttt{branchList} $\leftarrow$ \texttt{find\_branches}($\mathcal{C}^*$)\; 
		
		\For{$\mathcal{B}$ in \texttt{branchList}}{
			$\mathcal{C}^* \leftarrow$ \texttt{process}($\mathcal{B}$)\; 
		}
	
		\BlankLine
		
		\texttt{parallelList} $\leftarrow$ \texttt{find\_parallels}($\mathcal{C}^*$)\; 
	
		\For{$\mathcal{P}$ in \texttt{parallelList}}{
			$\mathcal{C}^* \leftarrow$ \texttt{process}($\mathcal{P}$)\; 
		}
	
		\BlankLine
		
		\texttt{loopList} $\leftarrow$ \texttt{find\_loops}($\mathcal{C}^*$)\; 
		
		\For{$\mathcal{L}$ in \texttt{loopList}}{
			$\mathcal{C}^* \leftarrow$ \texttt{process}($\mathcal{L}$)\; 
		}

		\BlankLine
		
		\texttt{exitList} $\leftarrow$ \texttt{find\_exits}($\mathcal{C}^*$)\; 
		
		\For{$\mathcal{E}$ in \texttt{exitList}}{
			$\mathcal{C}^* \leftarrow$ \texttt{process}($\mathcal{E}$)\; 
		}
	}
	\Return $\mathcal{C}^*$
\end{algorithm}











\section{Optimization Problem Formulations}

To achieve our goal consisting in to find the best choreography configuration capable to guarantee QoS constraints imposed by an user, we have to solve an optimization problem, which we have formulated in two very different manners, where:

\begin{itemize}
	\item the first one is based on the Multidimensional Knapsack Problem formulation.
	\item the second one is based on the multi-dimensional multi-choice knapsack problem formulation. 
\end{itemize}

In this section, we will describe and analyze both approach, despite only for the second one we have developed and implemented an heuristic approach capable to solve it in a faster way respect to exact algorithms. 

\subsection{Multidimensional Knapsack Problem Formulation}

Let $\mathcal{C} = (\Phi,E)$ a serverless choreography, $\textbf{X} \in \Omega$ a choreography configuration and $\left\langle (RT,w_{RT}),(C,w_{C}) \right\rangle$ the SLA imposed by an user regarding the execution of $\mathcal{C}$

At any time $t$, the \textit{score} $S(\textbf{X},t)$, or \textit{profit}, of the choreography configuration $\textbf{X}$ can be computed as follows:

\begin{equation}
	S(\textbf{X},t) \mathDef w_{RT} \cdot \dfrac{RT_{C_{\textbf{max}}}(t) - RT_C(\textbf{X},t)}{RT_{C_{\textbf{max}}}(t) - RT_{C_{\textbf{min}}}(t)} + w_{C} \cdot \dfrac{C_{C_{\textbf{max}}}(t) - C_C(\textbf{X},t)}{C_{C_{\textbf{max}}}(t) - C_{C_{\textbf{min}}}(t)}
\end{equation}

where:

\begin{itemize}
	\item $RT_{C_{\textbf{max}}}(t)$ ($RT_{C_{\textbf{min}}}(t)$) and $C_{C_{\textbf{max}}}(t)$ ($C_{C_{\textbf{min}}}(t)$) denote, respectively, the maximum (minimum) value for the overall expected response time and cost observed at time $t$.
\end{itemize}

A very naive formulation to our problem can be expressed in term of the the Multidimensional Knapsack Problem (MKP), a well-studied, strongly NP-hard combinatorial optimization problem occurring in many different applications, whose goal is to choose a subset of items with maximum total profit. Selected items must, however, not exceed resource capacities, which are called as knapsack constraints.

The MKP formulation for our problem can be defined by the following ILP:

\begin{align}
\displaystyle max \qquad & \displaystyle \sum_{\omega = 1}^{|\Omega|} x_{\omega} F(\textbf{X}_{\omega}) \\
\text{subject to} \qquad & \displaystyle \sum_{\omega = 1}^{|\Omega|} x_{\omega} C(\textbf{X}_{\omega}) \leq C_{user} \\
& \displaystyle \sum_{\omega = 1}^{|\Omega|} x_{\omega} RT(\textbf{X}_{\omega}) \leq RT_{user} \\ 
& \displaystyle \sum_{\omega = 1}^{|\Omega|} x_{\omega} = 1 & \\
& x_{\omega} \in \lbrace 0, 1 \rbrace & \qquad \forall \omega \in \N \cap [1,|\Omega|]
\end{align}

where:

Is very important to observe, that each column of above formulation represents the profit and resource requirements referring to a choreography configuration.

The number of existing choreography configuration (or columns above LP) is very large because it grows exponentially. Therefore, become impractical to generate and enumerate all possible choreography configuration.

For example, suppose to have a serverless choreography $\mathcal{C}$ made up of $6$ executable functions. Supposing to have $46$ possible memory choices, even if only one concrete function exist for each executable function, the solution space $\Omega$ will contain $9.47$ billion different configurations, making any exhaustive enumeration and search computationally unfeasible. 

Even if we had a way of generating all configurations, that is all columns of our problem, due to its complexity, any attempt to find the optimal solution rapidly will be an illusion. Moreover, it will be very likely that any computer runs out his memory.

\subsection{Multi-dimensional Multi-choice Knapsack Problem Formulation}

To avoid the enumeration of all possible choreography configuration, we have decided to use another kind of formulation, which is base on the Multidimensional Multiple-choice Knapsack Problem (MMKP), a variant of the previous one. 

Let $\mathcal{C} = (\Phi,E)$ a choreography and $\mathscr{F_E}(\mathcal{C})$ the set containing all executable functions of $\mathcal{C}$. Moreover, suppose that $|\mathscr{F_E}(\mathcal{C})| = n \in \N \setminus \left\{0\right\}$.

In this context, we have $n$ \textit{groups} of \textit{items} where, for $i \in \N \cap \left[1,n\right]$, the $i$-th group represents the set of all possible executable configurations for the executable function $\phi_i \in \mathscr{F_E}(\mathcal{C})$, which belong to the set $\left\{ \textbf{F}_{\phi_{i}} \times \N \right\}$, where $\textbf{F}_{\phi_{i}}$, we recall, is the implementation-set for $\phi_i$. Therefore, each $i$-th group contains $|\left\{ \textbf{F}_{\phi_{i}} \times \N \right\}|$ items, that is executable function configurations.

According to MMKP, our objective is to pick exactly one item from each group, that is exactly one configuration for each executable function $\phi \in \mathscr{F_E}(\mathcal{C})$, in such a way to maximize the profit value of the selected items, respecting several resource constraints of the knapsack, which, in our case, are represented by SLAs attributes values imposed by users. 



In this context, any $\xi \in \Xi$ represents a set of executable functions belonging to an unique combinations of parallel paths.

From now, we will use $\mathcal{\widetilde{P}}$ to denote the set containing all parallel structures of $\mathcal{C}$, while $\mathcal{\widetilde{P}}_i$ represents the $i$-th parallel structure belonging to $\mathcal{\widetilde{P}}$ for $i \in \N \cap \left[1;p \right]$, where $p = |\mathcal{\widetilde{P}}|$. Clearly, the notation $\mathscr{F_E}(\mathcal{\widetilde{P}})$ represents the set of all executable functions contained inside any parallel structure of a given choreography.

We will use $\Delta_{\mathcal{\widetilde{P}}_{i_j}}$ notation to represent the set containing all executable functions $\phi \in \mathscr{F_E}(\theta_{i_j})$, where $\theta_{i_j}$ is the $j$-th sub-choreography of the parallel structure $\mathcal{\widetilde{P}}_i$, which can be formally defined as follows:

\begin{equation}
	\Delta_{\mathcal{\widetilde{P}}_{i_j}} \mathDef \left\{ \phi \in \mathscr{F_E}(\mathcal{C}) \cap \mathscr{F_E}(\theta_{i_j}) : \text{  $\theta_{i_j}$ is the $j$-th sub-choreography of } \mathcal{\widetilde{P}}_i \right\} 
\end{equation}

In this context, $\Delta_{\mathcal{\widetilde{P}}_{i_j}}$ can be viewed as one of the parallel paths of $\mathcal{\widetilde{P}}_i$. Since the actual response time of any sub-choreography of $\mathcal{\widetilde{P}}_i$ depend on to the slower sub-choreography of $\mathcal{\widetilde{P}}_i$, can be not trivial to compute the profit and resource requirements for every function inside a parallel structure.  

Therefore, the main idea, regarding profit and resource requirements computation, is to verify if, for every parallel path for every parallel structure of the choreography, resource requirements are respected. 

We will use $\Xi$ to denote the set of all possible parallel paths combination inside a choreography. Formally it can be defined as follows: 

\begin{eqnarray}
	\Xi & \mathDef & \left\lbrace \Delta_{\mathcal{\widetilde{P}}_{1}}, \ldots, \Delta_{\mathcal{\widetilde{P}}_{p}} \right\rbrace \nonumber \\ 
	& \in & \left\{  \left\{ \bigcup_{j=1}^{|\mathcal{\widetilde{P}}_i|} \Delta_{\mathcal{\widetilde{P}}_{1_j}} \right\} \times \ldots \times \left\{ \bigcup_{j=1}^{|\mathcal{\widetilde{P}}_p|} \Delta_{\mathcal{\widetilde{P}}_{p_j}} \right\} \right\}  \nonumber \\
	& = & \Cross_{i = 1}^p  \left\{ \bigcup_{j=1}^{|\mathcal{\widetilde{P}}_i|} \Delta_{\mathcal{\widetilde{P}}_{i_j}} \right\}  \nonumber \\
\end{eqnarray}

\subsubsection{Formulation}


Before to formulate our MMKP instance, we have to explain how we can compute profit and resource requirements, expressed in term of response time and cost, for each executable function configuration belonging to each group, which is somewhat complicated.

Finally, wa are able to formulate our MMKP instance problem, which can be expressed as follows:

\begin{align}
	\label{MMKP}
	\displaystyle max \quad & \displaystyle \sum_{i = 1}^{|\mathscr{F_E}|}  \sum_{j = 1}^{|\textbf{F}_{\phi_{i}} \times \N|} y_{\phi_{i_{j}}} p_{\phi_{i_{j}}} & \\	
	\text{subject to} \quad  & \displaystyle \sum_{i = 1}^{|\mathscr{F_E}|}  \sum_{j = 1}^{|\textbf{F}_{\phi_{i}} \times \N|} y_{\phi_{i_{j}}} c_{\phi_{i_{j}}} \leq C_{user} & \\
	& \displaystyle \sum_{\phi_i \in \mathscr{F_E} \cap \mathscr{F_E}(\mathcal{\widetilde{P}})}  \sum_{j = 1}^{|\textbf{F}_{\phi_{i}} \times \N|} y_{\phi_{i_{j}}} t_{\phi_{i_{j}}} + \nonumber \\
	& \qquad + \sum_{\phi_h \in \xi}  \sum_{j = 1}^{|\textbf{F}_{\phi_{h}} \times \N|} y_{\phi_{h_{j}}} t_{\phi_{h_{j}}} \leq RT_{user} & 
	\forall \xi \in \Xi \\	
	& \displaystyle \sum_{\phi_i \in \mathscr{F_E} \cap \omega_{P_s}^{(l)}} \sum_{j = 1}^{|\textbf{F}_{\phi_{i}} \times \N|} y_{\phi_{i_{j}}} \leq l - r_s(t) & \forall s \in \N \cap \left[ 1, S \right] \\
	& \displaystyle \sum_{j = 1}^{|\textbf{F}_{\phi_{i}} \times \N|} y_{\phi_{i_{j}}} = 1 & \forall i \in \N \cap \left[ 1, |\mathscr{F_E}| \right] \\
	& y_{\phi_{i_{j}}} \in \lbrace 0, 1 \rbrace &
	\begin{array}{r}
		\forall i \in \N \cap \left[ 1, |\mathscr{F_E}| \right] \\ \forall j \in \N \cap [1,|\textbf{F}_{\phi_{i}} \times \N|]
	\end{array}
\end{align}


To simplify our notations, from now, we will use $o_{i_j}$ notation to denote the $i$-th item belonging to the $j$-th group, the latter denoted with $g_i$; clearly, in our context, the item $o_{i_j}$ coincides with the $j$-th executable function configuration $x_{\phi_{i_j}}$ for the executable function $\phi_i \in \mathscr{F_E}(\mathcal{C})$, for some $i \in \N \cap [1,|\mathscr{F_E}(\mathcal{C})|]$ and $j \in \N \cap [1,|\textbf{F}_{\phi_{i}} \times \N|]$. 

A solution of a MMKP is a set of selected objects $S \mathDef \left\{o_{1}, \ldots o_{n} \right\}$ where, clearly, an object $o_{i_j}$ is selected if the corresponding decision variable $y_{\phi_{i_j}}$ has been set to 1.


\subsubsection{The heuristic approach based on ACO}

Being MMKP an NP-hard problem, it is not always possible to find a feasible solution in a reasonable computing time especially for big instances. Although the \ref{MMKP}


 therefore an heuristic approach is needed for solving it. 

We have decided to built an algorithm based on \textit{Ant Colony Optimization} (ACO), a class of stochastic meta-heuristics that have been applied to solve many combinatorial optimization problems such as traveling salesman problems, quadratic assignment problems, or vehicle routing problems. 

Aforementioned class of meta-heuristics imitate the behavior shown by real ants when searching for food, where many simple interactions between single ants result in a very complex behavior of the whole ant colony.

Any ACO algorithm is based on a set of computational agent, called \textit{artificial ants}, which iteratively constructs a so-called  \textit{partial solution} for the instance to solve. At each iteration, each artificial ant moves from a partial solution to another, applying a series of stochastic local decisions whose policy is based on following parameters:

\begin{description}
	\item[Attractiveness] which is a value computed using an heuristic approach indicating the a \textit{priori} desirability of a given partial solution.
	
	\item[Pheromone Trail] which represents a value indicating how proficient it has been in the past to select a particular partial solution, representing, therefore, an a \textit{posteriori} indication of its desirability.
\end{description} 

In very simple terms, each ant incrementally constructs the partial solution to the problem and, after evaluating the find solution, it modifies the pheromone trail on the components used in its solution; that pheromone information will be use by future ants to find a better solutions to the problem instance. In fact, by increasing or decreasing the level of pheromone trails, ants distinguish "\textit{good}" from "\textit{bad}" solutions. In other words, pheromone trails represent the way according to which each ants communicate in order to find the best solution.
 
\subsubsection{Definition}

To solve MMKPs with ACO, we have to define how pheromone trails are laid by artificial ants, explaining how they follow them when constructing new partial solutions. In other words we need to decide which components of the constructed solutions should be rewarded, and how to exploit these rewards when constructing new solutions.

We define the \textit{solution's components graph} $\mathcal{G}=(\textbf{O},\textbf{E})$, on which ants lay pheromone trails, as a directed weighted graph such that:

\begin{equation}
	\textbf{O} \mathDef \bigcup_{j=1}^n \bigcup_{j=1}^{|g_i|} \left\{ o_{i_j} \right\}
\end{equation}

\begin{equation}
	(o_{i_j}, o_{k_m}) \in E \Leftrightarrow i \neq k \qquad \forall i,k \in \N \cap \left\{1,n\right\}, \forall m,j \in \N
\end{equation}

where: 

\begin{itemize}
	
	\item The set of vertices $\textbf{O}$ is made up all items belonging to each group. 
	
	Moreover, any object $o_{i_j}$ is adjacent to all other objects belonging to any group except those which belong to its same group.
	
	\item The weight $w \in \Rplus$ associated to each edge $(o_{i_j}, o_{k_m}) \in \textbf{E}$ is the value of the \textit{pheromone trail} laid by our artificial ants and it can change during algorithm execution. 
	
	The value of pheromone trail associated to each edge $(o_{i_j}, o_{k_m})$ during the $k$-th iteration of the algorithm is denoted as follows:
	
	\begin{equation}
		\tau_k(o_{i_j}, o_{k_m}) = w_k
	\end{equation}
	
	Intuitively, the pheromone value associated to the edge $(o_{i_j}, o_{k_m}) \in E$ represents the desirability to select the object $o_{k_m}$ when $o_{i_j}$ was been previously selected as part of the solution.
	
	We have adopted the so-called $\mathcal{MAX} - \mathcal{MIN}$ Ant System, according to which following constrain must be hold:
	
	\begin{equation}
		w_{\textbf{min}} \leq w_k \leq w_{\textbf{max}} 
	\end{equation}

	\begin{equation}
		\tau_0(o_{i_j}, o_{k_m}) = w_{\textbf{max}} \qquad \forall i,k \in \N \cap \left\{1,n\right\}, \forall m,j \in \N 
	\end{equation}
	
	that is, we explicitly impose lower and upper bounds $w_{\textbf{min}}$ and $w_{\textbf{max}}$ on pheromone
	trails, and pheromone trails are set to $w_{\textbf{max}}$ at the beginning of the search.
	
	
	the beginning of the search.
	At each cycle of this algorithm, every ant constructs a solution. 
	
	It first randomly chooses an initial object, and then iteratively adds objects that are chosen within a set Candidates that contains all the objects that can be selected without violating resource constraints. Once each ant has constructed a solution, pheromone trails are updated. 
	
	
\end{itemize}













\begin{algorithm}
	\KwData{this text}
	\KwResult{Some solution}
	Initialization pheromone trails\;
	\While{Termination conditions not met}{
		ConstructSolutions\;
		ApplyLocalSearch\;
		UpdatePheromoneTrails\;
	}
	\caption{Algorithmic skeleton for ACO algorithms}
\end{algorithm}


\begin{algorithm}
	%\KwData{this text}
	%\KwResult{Some solution}
	
	\For{$l\gets0$ \KwTo $m$ \KwBy $1$}{
		
		\BlankLine
		
		$z \leftarrow 0$ \;
		$\textbf{S}_{k_l}^{(z)} \leftarrow \emptyset $ \;
		$\textbf{G} \leftarrow \mathcal{G} $\;
		
		\BlankLine
		\BlankLine
		
		$\textbf{G}_i \leftarrow$ Randomly select a group from $\textbf{G}$\;
		$o_{i_j} \leftarrow$ Randomly select an object from $\textbf{G}_i$\;
		
		\BlankLine
		\BlankLine
		
		$z \leftarrow z + 1$ \;
		$\textbf{S}_{k_l}^{(z)} \leftarrow \{o_{i_j}\} $ \;
		$\textbf{G} \leftarrow \textbf{G} \setminus \textbf{G}_i $\;
		
		\BlankLine
		
		\While{$\textbf{G} \neq \emptyset$}{
			
			\BlankLine
			
			$\textbf{G}_a \leftarrow$ Randomly select a group from $\textbf{G}$\;
			
			\BlankLine	
			
			$\mathcal{O} \leftarrow $ Select a group of candidates objects belonging to $\textbf{G}_a$ which do not viotale resource constraints \;
			
			\BlankLine
			
			$o_{a_b} \leftarrow$ Select an object from $\textbf{G}_a$ having highest transition probability $P(o_{a_b}, \textbf{S}_{k_l}^{(z)}, \pi_{k_l}^{(z)})$\;
			
			\BlankLine	
			
			$z \leftarrow z + 1$\;
			
			\BlankLine	
			
			$\textbf{S}_{k_l}^{(z)} \leftarrow \textbf{S}_{k_l}^{(z)} \cup o_{a_b} $\;
			
			\BlankLine
			
			$\textbf{G} \leftarrow \textbf{G} \setminus \textbf{G}_a $\;
		}
	}
	
	$S_k \leftarrow$ Select $\textbf{S}_{k_l}^{(z)}$ having maximum profit\;
	
	\Return $S_k$
	\caption{Pseudo-code regarding partial solution generation performed by ants}
\end{algorithm}

\newpage

\subsubsection{Transition Probabilities}

Although firstly each ants selects randomly a object of his partial solution from a randomly chosen group, all subsequent objects are selected according to so-called \ItalicQuotMark{transition probability}, which depend on several parameters, like the 
attractiveness of the objects, the path traveled by the ant and the pheromone trail laid on the solution components graph $G_{S\textbf{}}$.

Let $z \in \SetFromOneTo{n-1}$ the number of items selected by an ant where $n$, we recall, represents the number of groups. Formally, during the $k$-th iteration of our algorithm, the path $\pi_{k_l}^{(z)}$, traversed by the $l$-th ant on the solution components graph $G_{\textbf{S}}(\textbf{O},E)$ after the $z$-th object selection, can be defined as follows:

\begin{equation}
	\pi_{k_l}^{(z)} = o_1e_1o_2 \ldots o_{z-1}e_{z}o_{z+1}
\end{equation} 

where:

\begin{eqnarray}
	o_s \in \textbf{O} & \qquad \forall s \in \SetFromOneTo{z} \\
	e_s = (o_s,o_{s+1}) \in E  & \qquad \forall s \in \SetFromOneTo{z} \\
	g(o_s) \neq g(o_r)   &\qquad \forall s,r \in \SetFromOneTo{z} : s \neq r
\end{eqnarray}

The last property state that each selected object belonging to distinct groups. We recall that the first object of $\pi_{k_l}^{(z)}$ is always picked randomly.

Moreover, $\textbf{S}_{k_l}^{(z)} = \left\{o_1,\ldots,o_{z+1}\right\}$ represents the partial solution built so far by the $l$-th ant after the $z$-th object selection; therefore, $|\textbf{S}_{k_l}^{(z)}| = z + 1$.  

Formally, after $z$ object selection, the transition probability associated to an object $o_{i_j} \in \textbf{O}$, given $\textbf{S}_{k_l}^{(z)}$ and $\pi_{k_l}^{(z)}$, can be computed as follows:

\begin{equation}
	P(o_{i_j}, \textbf{S}_{k_l}^{(z)}, \pi_{k_l}^{(z)}) \mathDef \frac{\left[ \tau( o_{i_j}, \pi_{k_l}^{(z)}) \right]^{\alpha} \cdot \left[ \eta( o_{i_j}, \textbf{S}_{k_l}^{(z)}) \right]^{\beta}}{\displaystyle \sum_{o_{i_j} \in \mathcal{C}(\textbf{G}_i, \textbf{S}_{k_l}^{(z)})} \left[ \tau( o_{i_j}, \pi_{k_l}^{(z)}) \right]^{\alpha} \cdot \left[ \eta( o_{i_j}, \textbf{S}_{k_l}^{(z)}) \right]^{\beta}}
\end{equation}

where:

\begin{itemize}
	\item $\tau( o_{i_j}, \pi_{k_l}^{(z)})$ is the pheromone factor.
	\item $\eta( o_{i_j}, \textbf{S}_{k_l}^{(z)})$ is the heuristic factor.
	\item $\alpha, \beta \in \Rplus$ are two parameter that determine, respectively, the relative importance of pheromone and heuristic factors.
\end{itemize}

The pheromone factor $\tau( o_{i_j}, \pi_{k_l}^{(z)})$ depends on the path $\pi_{k_l}^{(z)}$ and, in particular, on the quantity of pheromone laid on edges connecting the objects that already are in the partial solution $\textbf{S}_{k_l}^{(z)}$.

Formally, be $o_{z+1}$ the last vertex of the path $\pi_{k_l}^{(z)}$, the pheromone factor can be computed as follows:

\begin{equation}
	\tau( o_{i_j}, \pi_{k_l}^{(z)}) \mathDef \tau(o_{z+1}, o_{i_j}) + \sum_{e \in \pi_{k_l}^{(z)}} \tau(e) 
\end{equation}

The heuristic factor $\eta( o_{i_j}, \textbf{S}_{k_l}^{(z)})$ also depends on the whole set $\textbf{S}_{k_l}^{(z)}$ of selected objects. 



The following ratio:

\begin{equation}
	h_{\textbf{S}_{k_l}^{(z)}}(o_{i_j}) \mathDef \displaystyle \sum_{y=0}^{Y} \frac{c_{y_j}}{\displaystyle  b_y - \sum_{o_s \in \textbf{S}_{k_l}^{(z)}} c_y(o_z)}
\end{equation}

represents the tightness of the object $o_{i_j}$ on the problem's constraints relatively to the constructed solution $\textbf{S}_{k_l}^{(z)}$; thus, the lower this ratio is, the
more the object is profitable.

We can now define the heuristic factor formula as follows:

\begin{equation}
	\eta( o_{i_j}, \textbf{S}_{k_l}^{(z)}) \mathDef \frac{p_{i_j}}{h_{\textbf{S}_{k_l}^{(z)}}(o_{i_j})}
\end{equation}


\subsubsection{Local Search}

According to \ref{}, to make ACO algorithm competitive with  state-of-the-art algorithm for combinatorial optimization problems, several local search procedures must be used. These algorithms are invoked when solution construction phase is complete and $\textbf{S}_k$, that is the best solution built by ants, is obtained. The aim of that algorithms is to improve current iteration best solution, performing a local search, 

We have adopted two different local:

\begin{description}
	\item[Random Local Search]  an exhaustive search within a group to improve the solution. 
	
	It replaces current selected object of a group with every other object that do not violate resource constraints and checks if it is a better solution. 
	
	The total procedure is repeated a number of times, each time for a random group.
	
	\item[Random Item Swap] is an extended version of the
	random local search. 
	
	In this case, a specified number ($> 1$) of objects are swapped with other random objects from the same group without checking the resource constraints, then it checks if it is a valid solution and if it improves the solution.
\end{description}

\begin{algorithm}
	
	\For{a specified number of times}{
		
		\BlankLine
		
		$\textbf{G}_i \leftarrow$ Randomly select a group from $\mathcal{G}$\;
		
		\For{each object $o_{i_j} \in \textbf{G}_i$ other than the one in $\textbf{S}_k$}{
			
			\BlankLine
			
			$o_{i_s} \leftarrow $ The object belonging to $\textbf{S}_k$ such that $g(o_{i_s}) = g(o_{i_j})$
			
			\BlankLine
			
			$\textbf{S}_k^{(temp)} \leftarrow \left\{ \textbf{S}_k \setminus \left\{ o_{i_s} \right\} \right\} \cup \left\{ o_{i_j} \right\}$ 
			
			\BlankLine
			\BlankLine
			
			\If{$\textbf{S}_k^{(temp)}$ not violate MMKP constraints}{
				
				\BlankLine
				\BlankLine
				
				\If{$profit(\textbf{S}_k^{(temp)}) > profit(\textbf{S}_k) $}{
					$\textbf{S}_k \leftarrow \textbf{S}_k^{(temp)}$
				}
				
			}
		}	
	}
	\Return $\textbf{S}_k$
\end{algorithm}

\subsubsection{Pheromone Trail Update}

Let $m \in \N \SetMinusZero$ the amount of ants, $l \in \SetFromOneTo{m}$, $S_{best}$ the best solution, having maximal profit, constructed from the beginning of the algorithm and $\textbf{S}_k = \left\{S_{k_1}, \ldots, S_{k_m} \right\}$ the set containing all partial solutions constructed by ants during $k$-th iteration, where $S_{k_l}$ is the partial solution found by $l$-th ant. Finally, if the $l$-th ant fails to discover a partial solution, we will assume that $S_{k_l} = \emptyset$. 

For any $a,b \in \N$ and $i,j \in \SetFromOneTo{n}$ such that $i \neq j$, during $k$-th iteration of the algorithm, when solutions construction phase is complete, all pheromone trails associated to any edge $(o_{i_j}, o_{a_b})$, belonging to the solution components graph $G_{\textbf{S}}$, are updated using following formula:

\begin{equation}
	\tau(o_{i_a}, o_{j_b})_k = \tau(o_{i_a}, o_{j_b})_{k-1} \cdot \rho + \Delta \tau(o_{i_a}, o_{j_b})_{k} 
\end{equation}

where:

\begin{itemize}
	
	\item $\rho \in \mathbb{R} \cap \left[0,1\right]$ represents the so called \textit{evaporation coefficient}. 
	
	That coefficient is involved into the performing of a a mechanism called \textit{evaporation} according to which, lowering the pheromone trails by a constant factor, is possible to avoid unlimited accumulation of pheromone trail over components, decreasing their desirability during future algorithm's iterations.
	
	\item $\Delta \tau(o_{i_a}, o_{j_b})_{k}$ is the amount of pheromones laid on edge $(o_{i_a}, o_{j_b})$ by all ants who have traversed that edge for their solution construction. It can be computed as follows:
	
	\begin{equation}
		\Delta \tau(o_{i_a}, o_{j_b})_{k} \mathDef \sum_{l=1}^{m} \tau(o_{i_a}, o_{j_b})_k^{(l)}
	\end{equation}
	
	where: 
	
	\begin{itemize}
		\item $\tau(o_{i_a}, o_{j_b})_k^{(l)}$ is the amount of pheromones laid on edge $(o_{i_a}, o_{j_b})$ by $l$-th ant, which, letting $\pi_{k_l}$ the path traversed by that ant, can be computed as follows:
		
		\begin{equation}
			\tau(o_{i_j}, o_{a_b})_{k-1}^{l} \mathDef \left\{ 
			\begin{array}{ll}
				0 & \text{\textbf{if }} S_{k_l} = \emptyset \\ 
				\displaystyle \frac{1}{1 + profit(S_{best}) - profit(S_{k_l})} & \text{\textbf{if }} S_{k_l} \neq \emptyset \wedge (o_{i_j}, o_{a_b}) \in \pi_{k_l} \\ 
			\end{array} \right.
		\end{equation}
		
		
	\end{itemize}
\end{itemize}

\subsubsection{Termination Conditions}

Since for each item $o_{i_j}$ is true that $0 \leq p_{i_j} \leq 1$, 

The algorithm stops either when an ant has found an optimal solution (when the optimal bound is known), or when a maximum number of cycles has been performed.



\newpage


\chapter{Computational Model}





We have adopted for follwoing reasons:

\begin{enumerate}
	\item We are able to guarantee access transparency. 
	
	In fact, hiding any difference regarding serverless compound functions representation and the way according to which the latter can be accessed, reaching an agreement on how a compound function is to be represented among different FaaS providers, we allow our users to access to any serverless function hosted on any supported provider using identical operations.
	
	\item We can significantly reduce the impact of vendor lock-in problem. 
	
	Providing an unique way to represent a serverless compound functions, if our final users wish, they will be able to use serverless functions hosted on another FaaS provider without rewriting their serverless compound functions entirely, because very little changes to FC representation code are needed to complete the switching process. 
	
	In that way, guaranteeing reduced switching costs, using our system is possible to mitigate provider lock-in.
	
\end{enumerate}

\section{Abstract Function Choreography Language}

To overcome these weaknesses, we introduce a new Abstract
Function Choreography Language (AFCL), which is a novel approach
to specify FCs at a high-level of abstraction. 



From implementation point of view, our AFCL parser implementation is completely indepented from 
garanteeing low level coupling betweeng AFLC parser and choreography implementation.



\section{Pr}

Data collection is a major bottleneck in machine learning and an active res


f there are no existing datasets that can be used for training,
then another option is to generate the datasets either manu-
ally or automatically. For manual construction, crowdsourc-
ing is the standard method where human workers are given
tasks to gather the necessary bits of data that collectively
become the generated dataset. Alternatively, automatic tech-
niques can be used to generate synthetic datasets. Note that
data generation can also be viewed as data augmentation if
there is existing data where some missing parts needs to be
filled in.

\subsection{InfluxDB}

A very important characteristic of our data-set it that it contains time series data, where the time of each instance, containing the attribute value regarding power consumption, is given by a timestamp attribute; thus our data-set represents a sequence of discrete-
time data [8]. All data are listed in time order.


InfluxDB is a TSDB that stores \textit{points}, that is single values indexed by time. 

Using InfluxDB terminology, each point is uniquely identified by four components:

\begin{itemize}
	\item A timestamp.
	
	\item Zero o more tags, key-value pairs that are used to store metadata associated with the point. 
	 
	\item One or more fields, that is scalars which represent the value recorded by that point.
	
	\item A measurement, which acts as a container used to group together all related points.
	
\end{itemize}

Is very important to note that  

In our implementation, data points 

A bucket is a named location where time series data is stored.





\bibliographystyle{plain}
\bibliography{Bibliography.bib}



\end{document}